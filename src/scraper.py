"""
ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ë§ ëª¨ë“ˆ
APIë¥¼ í†µí•´ ë§¤ë¬¼ ì •ë³´ ìˆ˜ì§‘
"""

import requests
import random
import time
from typing import List, Dict, Optional
import logging
from datetime import datetime
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NaverRealEstateScraper:
    """ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    # ë¸Œë¼ìš°ì €ë³„ ì™„ì „í•œ í”„ë¡œíŒŒì¼ (Fingerprinting ìš°íšŒ)
    BROWSER_PROFILES = [
        # Chrome 121 (Windows) - ìµœì‹  ë²„ì „
        {
            'type': 'chrome',
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'sec_ch_ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'sec_ch_ua_mobile': '?0',
            'sec_ch_ua_platform': '"Windows"',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Chrome 120 (Windows)
        {
            'type': 'chrome',
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'sec_ch_ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec_ch_ua_mobile': '?0',
            'sec_ch_ua_platform': '"Windows"',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Chrome 121 (Mac)
        {
            'type': 'chrome',
            'user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'sec_ch_ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'sec_ch_ua_mobile': '?0',
            'sec_ch_ua_platform': '"macOS"',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Firefox 122 (Windows)
        {
            'type': 'firefox',
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Firefox 121 (Windows)
        {
            'type': 'firefox',
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Firefox 122 (Mac)
        {
            'type': 'firefox',
            'user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Edge 121 (Windows)
        {
            'type': 'edge',
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',
            'sec_ch_ua': '"Not A(Brand";v="99", "Microsoft Edge";v="121", "Chromium";v="121"',
            'sec_ch_ua_mobile': '?0',
            'sec_ch_ua_platform': '"Windows"',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Safari 17.2 (Mac)
        {
            'type': 'safari',
            'user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
            'accept': 'application/json, text/javascript, */*; q=0.01',
            'accept_language': 'ko-KR,ko;q=0.9',
        },
    ]
    
    BASE_URL = "https://new.land.naver.com"
    
    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.current_browser_profile = None  # í˜„ì¬ ë¸Œë¼ìš°ì € í”„ë¡œíŒŒì¼
        self.cookies_received = False  # ì¿ í‚¤ ìˆ˜ì‹  ì—¬ë¶€
        self.last_cookie_refresh = time.time()  # ë§ˆì§€ë§‰ ì¿ í‚¤ ê°±ì‹  ì‹œê°„
        
        self._update_headers()
        self._visit_homepage()  # ì´ˆê¸° ë°©ë¬¸ìœ¼ë¡œ ì¿ í‚¤ ë°›ê¸°
        
        # ì‚¬ëŒì²˜ëŸ¼ í–‰ë™í•˜ê¸° ìœ„í•œ ìƒíƒœ ê´€ë¦¬
        self.request_count = 0  # ì´ ìš”ì²­ íšŸìˆ˜
        self.last_break_count = 0  # ë§ˆì§€ë§‰ íœ´ì‹ ì‹œì 
        self.session_start_time = time.time()  # ì„¸ì…˜ ì‹œì‘ ì‹œê°„
        self.fatigue_level = 0.0  # í”¼ë¡œë„ (0.0 ~ 1.0)
    
    def _visit_homepage(self):
        """
        ë„¤ì´ë²„ ë¶€ë™ì‚° í™ˆí˜ì´ì§€ ë°©ë¬¸ (ì¿ í‚¤ ë°›ê¸°)
        ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë™ì‘í•˜ê¸° ìœ„í•´
        
        ì¤‘ìš”: ì´ ê³¼ì •ì—ì„œ NNB, JSESSIONID ë“± ë„¤ì´ë²„ ì¿ í‚¤ë¥¼ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤!
        """
        try:
            logger.info("ğŸŒ ë„¤ì´ë²„ ë¶€ë™ì‚° ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ ì¤‘ (ì¿ í‚¤ ìˆ˜ì‹ )...")
            
            # Accept í—¤ë”ë¥¼ HTML í˜ì´ì§€ìš©ìœ¼ë¡œ ë³€ê²½
            original_accept = self.session.headers.get('Accept', '')
            self.session.headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'
            
            # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
            response = self.session.get(self.BASE_URL, timeout=10)
            
            # ì¿ í‚¤ ìˆ˜ì‹  í™•ì¸
            cookies = self.session.cookies.get_dict()
            if cookies:
                self.cookies_received = True
                self.last_cookie_refresh = time.time()
                logger.info(f"âœ… ì¿ í‚¤ ìˆ˜ì‹  ì„±ê³µ: {len(cookies)}ê°œ")
                
                # ì£¼ìš” ì¿ í‚¤ ë¡œê¹… (NNB, JSESSIONID ë“±)
                important_cookies = ['NNB', 'JSESSIONID', 'nid_inf', 'NID_AUT', 'NID_SES']
                found_cookies = [key for key in important_cookies if key in cookies]
                if found_cookies:
                    logger.info(f"ğŸª ì£¼ìš” ì¿ í‚¤ í™•ì¸: {', '.join(found_cookies)}")
                else:
                    logger.warning("âš ï¸  ì£¼ìš” ì¿ í‚¤(NNB, JSESSIONID)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                logger.warning("âš ï¸  ì¿ í‚¤ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì°¨ë‹¨ë  ê°€ëŠ¥ì„± ë†’ìŒ!")
            
            # Accept í—¤ë” ë³µì›
            if original_accept:
                self.session.headers['Accept'] = original_accept
            
            time.sleep(random.uniform(2, 4))
            logger.info("âœ… ì´ˆê¸° ë°©ë¬¸ ì™„ë£Œ (ì„¸ì…˜ ì¤€ë¹„ë¨)")
            
        except Exception as e:
            logger.warning(f"âŒ ì´ˆê¸° ë°©ë¬¸ ì‹¤íŒ¨: {e}")
    
    def _update_headers(self):
        """
        ìš”ì²­ í—¤ë” ì—…ë°ì´íŠ¸ (Fingerprinting ì™„ë²½ ìš°íšŒ)
        ë¸Œë¼ìš°ì €ë³„ë¡œ ì™„ì „íˆ ë‹¤ë¥¸ í—¤ë” í”„ë¡œíŒŒì¼ ì‚¬ìš©
        """
        # ë¸Œë¼ìš°ì € í”„ë¡œíŒŒì¼ ë¬´ì‘ìœ„ ì„ íƒ
        self.current_browser_profile = random.choice(self.BROWSER_PROFILES)
        browser_type = self.current_browser_profile['type']
        
        # ê¸°ë³¸ í—¤ë” (ëª¨ë“  ë¸Œë¼ìš°ì € ê³µí†µ)
        headers = {
            'Host': 'new.land.naver.com',  # ëª…ì‹œì  ì„¤ì • (ì¤‘ìš”!)
            'User-Agent': self.current_browser_profile['user_agent'],
            'Accept': self.current_browser_profile['accept'],
            'Accept-Language': self.current_browser_profile['accept_language'],
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://new.land.naver.com/',
            'Origin': 'https://new.land.naver.com',
            'Connection': 'keep-alive',
            'DNT': '1',  # Do Not Track
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }
        
        # Chrome/Edge ì „ìš© í—¤ë” (Sec-Fetch-*, sec-ch-ua)
        if browser_type in ['chrome', 'edge']:
            headers.update({
                'sec-ch-ua': self.current_browser_profile['sec_ch_ua'],
                'sec-ch-ua-mobile': self.current_browser_profile['sec_ch_ua_mobile'],
                'sec-ch-ua-platform': self.current_browser_profile['sec_ch_ua_platform'],
                'Sec-Fetch-Site': 'same-origin',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Dest': 'empty',
            })
        
        # Firefox ì „ìš© í—¤ë”
        elif browser_type == 'firefox':
            headers.update({
                'TE': 'trailers',  # Firefox ê³ ìœ 
            })
        
        # Safari ì „ìš© í—¤ë”
        elif browser_type == 'safari':
            # SafariëŠ” sec-ch-ua ì—†ìŒ
            pass
        
        self.session.headers.clear()
        self.session.headers.update(headers)
        
        logger.info(f"ğŸŒ ë¸Œë¼ìš°ì € í”„ë¡œíŒŒì¼ ë³€ê²½: {browser_type.upper()} - {self.current_browser_profile['user_agent'][:50]}...")
    
    def _human_like_delay(self, base_min_minutes: float = 1.0, base_max_minutes: float = 3.0) -> float:
        """
        ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•œ ëŒ€ê¸° ì‹œê°„ ìƒì„± (ë¶„ ë‹¨ìœ„, ì •ê·œë¶„í¬ ì‚¬ìš©)
        
        Args:
            base_min_minutes: ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ë¶„)
            base_max_minutes: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ë¶„)
            
        Returns:
            ì‹¤ì œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        # ì •ê·œë¶„í¬ë¡œ ë” ìì—°ìŠ¤ëŸ¬ìš´ ëœë¤ì„±
        mean = (base_min_minutes + base_max_minutes) / 2
        std = (base_max_minutes - base_min_minutes) / 4
        delay_minutes = np.random.normal(mean, std)
        
        # ìµœì†Œ/ìµœëŒ€ ë²”ìœ„ ë‚´ë¡œ ì œí•œ
        delay_minutes = max(base_min_minutes, min(base_max_minutes, delay_minutes))
        
        # í”¼ë¡œë„ ë°˜ì˜ (ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ëŠë ¤ì§)
        delay_minutes *= (1 + self.fatigue_level * 0.5)
        
        # í™œë™ ì‹œê°„ëŒ€ ë°˜ì˜ (ë‚® ì‹œê°„ vs ë°¤ ì‹œê°„)
        hour = datetime.now().hour
        if 9 <= hour <= 18:  # ì˜¤ì „ 9ì‹œ ~ ì˜¤í›„ 6ì‹œ (í™œë°œ)
            delay_minutes *= 0.8
        elif hour >= 22 or hour <= 6:  # ë°¤ 10ì‹œ ~ ìƒˆë²½ 6ì‹œ (ëŠë¦¼)
            delay_minutes *= 1.3
        
        # ë¶„ì„ ì´ˆë¡œ ë³€í™˜
        return delay_minutes * 60
    
    def _should_take_break(self) -> bool:
        """
        íœ´ì‹ì´ í•„ìš”í•œì§€ íŒë‹¨ (ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•˜ê²Œ)
        
        Returns:
            True if íœ´ì‹ í•„ìš”
        """
        requests_since_break = self.request_count - self.last_break_count
        
        # 5-10ê°œ ìš”ì²­ë§ˆë‹¤ íœ´ì‹ (ëœë¤)
        break_threshold = random.randint(5, 10)
        
        if requests_since_break >= break_threshold:
            # 80% í™•ë¥ ë¡œ íœ´ì‹ (ì™„ì „íˆ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ê²Œ)
            return random.random() < 0.8
        
        # ê°€ë” ê°‘ìê¸° íœ´ì‹ (5% í™•ë¥ )
        return random.random() < 0.05
    
    def _take_break(self):
        """
        ê¸´ íœ´ì‹ ì‹œê°„ (ì‚¬ëŒì´ ì»¤í”¼ ë§ˆì‹œê±°ë‚˜ ì ì‹¬ ë¨¹ëŠ” ì‹œê°„) - ë¶„ ë‹¨ìœ„
        """
        # ë² íƒ€ ë¶„í¬ë¡œ ë” ìì—°ìŠ¤ëŸ¬ìš´ íœ´ì‹ ì‹œê°„ (10ë¶„~30ë¶„, í‰ê·  20ë¶„)
        alpha, beta = 2, 2
        normalized = np.random.beta(alpha, beta)
        break_minutes = 10 + normalized * 20  # 10~30ë¶„
        break_seconds = break_minutes * 60
        
        self.last_break_count = self.request_count
        
        logger.info(f"â˜• ì¥ì‹œê°„ íœ´ì‹ (ì ì‹¬/ì»¤í”¼): {break_minutes:.1f}ë¶„ ({break_seconds:.0f}ì´ˆ) ëŒ€ê¸°...")
        logger.info(f"   (ì´ {self.request_count}ê°œ ìš”ì²­ ì™„ë£Œ, í”¼ë¡œë„: {self.fatigue_level:.2f})")
        
        time.sleep(break_seconds)
        
        # íœ´ì‹ í›„ í”¼ë¡œë„ ê°ì†Œ
        self.fatigue_level = max(0, self.fatigue_level - 0.2)
    
    def _simulate_mouse_movement(self):
        """
        ë§ˆìš°ìŠ¤ ì›€ì§ì„ ì‹œë®¬ë ˆì´ì…˜ (íŠ¹ì • ì¢Œí‘œë¡œ ë¶€ë“œëŸ½ê²Œ ì´ë™)
        """
        # ë§ˆìš°ìŠ¤ë¥¼ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë‚˜ëˆ  ë¶€ë“œëŸ½ê²Œ ì´ë™ (5-10ë‹¨ê³„)
        steps = random.randint(5, 10)
        
        logger.info(f"ğŸ–±ï¸  ë§ˆìš°ìŠ¤ ì›€ì§ì„ ì‹œë®¬ë ˆì´ì…˜ ({steps}ë‹¨ê³„)...")
        
        for i in range(steps):
            # ê° ë‹¨ê³„ë§ˆë‹¤ 0.1~0.5ì´ˆ ëŒ€ê¸°
            step_delay = random.uniform(0.1, 0.5)
            time.sleep(step_delay)
    
    def _simulate_scroll(self):
        """
        í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜ (ìœ„ì•„ë˜ ë¶ˆê·œì¹™í•˜ê²Œ)
        """
        # ìŠ¤í¬ë¡¤ íšŸìˆ˜ (2-5íšŒ)
        scroll_count = random.randint(2, 5)
        
        logger.info(f"ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜ ({scroll_count}íšŒ)...")
        
        for i in range(scroll_count):
            # ê° ìŠ¤í¬ë¡¤ë§ˆë‹¤ 0.3~1.0ì´ˆ ëŒ€ê¸°
            scroll_delay = random.uniform(0.3, 1.0)
            time.sleep(scroll_delay)
            
            # ê°€ë” ìœ„ë¡œ ìŠ¤í¬ë¡¤ (20% í™•ë¥ )
            if random.random() < 0.2:
                logger.info(f"   â†‘ ìœ„ë¡œ ìŠ¤í¬ë¡¤")
            else:
                logger.info(f"   â†“ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤")
    
    def _simulate_reading(self):
        """
        í˜ì´ì§€ë¥¼ ì½ëŠ” ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ìŠ¤í¬ë¡¤, í´ë¦­ ë“±) - ë¶„ ë‹¨ìœ„
        """
        # ë§ˆìš°ìŠ¤ ì›€ì§ì„ + ìŠ¤í¬ë¡¤ + ì½ê¸°
        self._simulate_mouse_movement()
        self._simulate_scroll()
        
        # ê°ë§ˆ ë¶„í¬ë¡œ ì½ê¸° ì‹œê°„ (1ë¶„~5ë¶„, í‰ê·  2.5ë¶„)
        reading_minutes = np.random.gamma(2, 1.5)
        reading_minutes = min(5, max(1, reading_minutes))
        reading_seconds = reading_minutes * 60
        
        logger.info(f"ğŸ“– ë§¤ë¬¼ ìƒì„¸ ì½ëŠ” ì¤‘... {reading_minutes:.1f}ë¶„ ({reading_seconds:.0f}ì´ˆ)")
        time.sleep(reading_seconds)
    
    def _update_fatigue(self):
        """
        í”¼ë¡œë„ ì—…ë°ì´íŠ¸ (ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ì¦ê°€)
        """
        session_duration = (time.time() - self.session_start_time) / 3600  # ì‹œê°„ ë‹¨ìœ„
        self.fatigue_level = min(1.0, session_duration * 0.1)  # 10ì‹œê°„ í›„ ìµœëŒ€
    
    def _check_and_refresh_cookies(self):
        """
        ì¿ í‚¤ ìœ íš¨ì„± ê²€ì‚¬ ë° í•„ìš”ì‹œ ì¬ë°©ë¬¸
        
        ë„¤ì´ë²„ ì¿ í‚¤ëŠ” ì‹œê°„ì´ ì§€ë‚˜ë©´ ë§Œë£Œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
        ì¼ì • ì‹œê°„(30ë¶„)ë§ˆë‹¤ ë©”ì¸ í˜ì´ì§€ë¥¼ ë‹¤ì‹œ ë°©ë¬¸í•˜ì—¬ ì¿ í‚¤ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.
        """
        # 30ë¶„(1800ì´ˆ)ë§ˆë‹¤ ì¿ í‚¤ ê°±ì‹ 
        cookie_lifetime = 1800  # 30ë¶„
        current_time = time.time()
        
        if not self.cookies_received or (current_time - self.last_cookie_refresh) > cookie_lifetime:
            logger.info("ğŸ”„ ì¿ í‚¤ ë§Œë£Œ ë˜ëŠ” ë¯¸ìˆ˜ì‹  â†’ ë©”ì¸ í˜ì´ì§€ ì¬ë°©ë¬¸...")
            self._visit_homepage()
    
    def _visit_landing_page(self, page_type: str):
        """
        API í˜¸ì¶œ ì „ì— í•´ë‹¹ í˜ì´ì§€ë¥¼ ë¨¼ì € ë°©ë¬¸ (ëœë”© í˜ì´ì§€ ì „ëµ)
        
        Args:
            page_type: 'complexes' (ë‹¨ì§€ ëª©ë¡), 'complex' (ë‹¨ì§€ ìƒì„¸), 'articles' (ë§¤ë¬¼ ëª©ë¡)
        """
        landing_urls = {
            'complexes': 'https://new.land.naver.com/complexes',
            'complex': 'https://new.land.naver.com/complexes',
            'articles': 'https://new.land.naver.com/articles',
        }
        
        landing_url = landing_urls.get(page_type, self.BASE_URL)
        
        try:
            logger.info(f"ğŸšª ëœë”© í˜ì´ì§€ ë°©ë¬¸: {landing_url}")
            
            # Accept í—¤ë”ë¥¼ HTML í˜ì´ì§€ìš©ìœ¼ë¡œ ë³€ê²½
            original_accept = self.session.headers.get('Accept', '')
            self.session.headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8'
            
            # í˜ì´ì§€ ë°©ë¬¸
            self.session.get(landing_url, timeout=10)
            
            # Accept í—¤ë” ë³µì› (API ìš”ì²­ìš©)
            if original_accept:
                self.session.headers['Accept'] = original_accept
            
            # ì§§ì€ ëŒ€ê¸° (0.5-1.5ì´ˆ)
            time.sleep(random.uniform(0.5, 1.5))
            
        except Exception as e:
            logger.warning(f"âš ï¸  ëœë”© í˜ì´ì§€ ë°©ë¬¸ ì‹¤íŒ¨: {e}")
    
    def _get_referer_for_url(self, url: str) -> str:
        """
        URLì— ë”°ë¼ ì ì ˆí•œ Referer ë°˜í™˜ (Referer ì²´ì¸)
        
        Args:
            url: ìš”ì²­ URL
            
        Returns:
            ì ì ˆí•œ Referer URL
        """
        if '/api/complexes' in url:
            # ë‹¨ì§€ ê²€ìƒ‰ API â†’ ë©”ì¸ í˜ì´ì§€ì—ì„œ ì˜¨ ê²ƒì²˜ëŸ¼
            return 'https://new.land.naver.com/'
        elif '/api/articles/complex/' in url:
            # ë§¤ë¬¼ ëª©ë¡ API â†’ ë‹¨ì§€ í˜ì´ì§€ì—ì„œ ì˜¨ ê²ƒì²˜ëŸ¼
            return 'https://new.land.naver.com/complexes'
        elif '/api/articles/' in url:
            # ë§¤ë¬¼ ìƒì„¸ API â†’ ë§¤ë¬¼ ëª©ë¡ì—ì„œ ì˜¨ ê²ƒì²˜ëŸ¼
            return 'https://new.land.naver.com/articles'
        else:
            # ê¸°ë³¸ê°’
            return 'https://new.land.naver.com/'
    
    def _safe_request(self, url: str, params: Dict = None, retry: int = 3) -> Optional[Dict]:
        """
        ì•ˆì „í•œ HTTP ìš”ì²­ (ì¬ì‹œë„ í¬í•¨, 429 ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬, ì‚¬ëŒì²˜ëŸ¼ í–‰ë™)
        
        Args:
            url: ìš”ì²­ URL
            params: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
            retry: ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            JSON ì‘ë‹µ ë˜ëŠ” None
        """
        # ì¿ í‚¤ ìœ íš¨ì„± ê²€ì‚¬ ë° ê°±ì‹ 
        self._check_and_refresh_cookies()
        
        # ìš”ì²­ ì „ íœ´ì‹ í•„ìš” ì—¬ë¶€ í™•ì¸
        if self._should_take_break():
            self._take_break()
        
        for attempt in range(retry):
            try:
                # ìš”ì²­ë§ˆë‹¤ User-Agent ë³€ê²½ (ë‹¤ì–‘í•œ ë¸Œë¼ìš°ì € ì‚¬ìš©)
                self._update_headers()
                
                # URLì— ë§ëŠ” Referer ì„¤ì •
                referer = self._get_referer_for_url(url)
                self.session.headers['Referer'] = referer
                
                # ì¿ í‚¤ ìƒíƒœ ë¡œê¹… (ë””ë²„ê¹…ìš©)
                if self.request_count % 10 == 0:  # 10ë²ˆë§ˆë‹¤
                    cookies_count = len(self.session.cookies.get_dict())
                    logger.info(f"ğŸª í˜„ì¬ ì¿ í‚¤ ìˆ˜: {cookies_count}ê°œ")
                
                # ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•œ ëŒ€ê¸° (ë¶„ ë‹¨ìœ„, ì •ê·œë¶„í¬)
                if attempt == 0:
                    delay = self._human_like_delay(1.0, 3.0)  # 1-3ë¶„
                    delay_minutes = delay / 60
                    logger.info(f"ğŸ¤” ìƒê°í•˜ëŠ” ì¤‘... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
                    time.sleep(delay)
                
                # ìš”ì²­ ì¹´ìš´íŠ¸ ì¦ê°€ ë° í”¼ë¡œë„ ì—…ë°ì´íŠ¸
                self.request_count += 1
                self._update_fatigue()
                
                response = self.session.get(url, params=params, timeout=30)
                
                if response.status_code == 200:
                    # ì„±ê³µ ì‹œ í˜ì´ì§€ ì½ê¸° ì‹œë®¬ë ˆì´ì…˜ (30% í™•ë¥ )
                    if random.random() < 0.3:
                        self._simulate_reading()
                    return response.json()
                
                elif response.status_code == 429:
                    # 429 Too Many Requests - 30ë¶„ ëŒ€ê¸°!
                    wait_minutes = 30
                    wait_seconds = wait_minutes * 60
                    
                    logger.warning(f"âš ï¸  429 ì—ëŸ¬ (Too Many Requests) ë°œìƒ!")
                    logger.info(f"ğŸš¨ í¬ë¡¤ë§ ì°¨ë‹¨ ê°ì§€ - {wait_minutes}ë¶„ ({wait_seconds}ì´ˆ) ëŒ€ê¸°...")
                    logger.info(f"   í˜„ì¬ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    logger.info(f"   ì¬ê°œ ì˜ˆì •: {(datetime.now().timestamp() + wait_seconds)}")
                    
                    if attempt < retry - 1:
                        logger.info(f"ğŸ• {wait_minutes}ë¶„ íœ´ì‹ í›„ ì¬ì‹œë„ ì˜ˆì • ({attempt + 2}/{retry})")
                        time.sleep(wait_seconds)
                        self._update_headers()  # User-Agent ë³€ê²½
                        logger.info("âœ… íœ´ì‹ ì™„ë£Œ. í¬ë¡¤ë§ ì¬ê°œ...")
                    else:
                        logger.error("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. í”„ë¡œê·¸ë¨ì„ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
                        return None
                
                elif response.status_code == 403:
                    # 403 Forbidden
                    logger.warning(f"ì ‘ê·¼ ê±°ë¶€ (403). í—¤ë” ë³€ê²½ í›„ ì¬ì‹œë„... ({attempt + 1}/{retry})")
                    self._update_headers()
                    delay = random.uniform(5, 10)
                    time.sleep(delay)
                
                else:
                    logger.warning(f"ì‘ë‹µ ì½”ë“œ {response.status_code}")
                    if attempt < retry - 1:
                        delay = random.uniform(3, 7)
                        time.sleep(delay)
                
            except requests.exceptions.RequestException as e:
                logger.error(f"ìš”ì²­ ì˜¤ë¥˜: {e}")
                if attempt < retry - 1:
                    delay = random.uniform(5, 10)
                    logger.info(f"ì˜¤ë¥˜ í›„ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(delay)
        
        return None
    
    def search_complexes(self, cortarNo: str, trade_type: str = "A1") -> List[Dict]:
        """
        ì§€ì—­ë³„ ë‹¨ì§€ ê²€ìƒ‰
        
        Args:
            cortarNo: ì§€ì—­ ì½”ë“œ (ì˜ˆ: 1168010600 - ê°•ë‚¨êµ¬ ëŒ€ì¹˜ë™)
            trade_type: ê±°ë˜ ìœ í˜• (A1: ë§¤ë§¤, B1: ì „ì„¸, B2: ì›”ì„¸, B3: ë‹¨ê¸°ì„ëŒ€)
            
        Returns:
            ë‹¨ì§€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        # API í˜¸ì¶œ ì „ ëœë”© í˜ì´ì§€ ë¨¼ì € ë°©ë¬¸ (ì¤‘ìš”!)
        self._visit_landing_page('complexes')
        
        url = f"{self.BASE_URL}/api/complexes"
        
        params = {
            'cortarNo': cortarNo,
            'realEstateType': 'APT:OPST',  # ì•„íŒŒíŠ¸, ì˜¤í”¼ìŠ¤í…”
            'tradeType': trade_type,
            'tag': '::::::::',
            'rentPriceMin': 0,
            'rentPriceMax': 999999,
            'priceMin': 0,
            'priceMax': 999999,
            'areaMin': 0,
            'areaMax': 999999,
            'oldBuildYears': '',
            'recentlyBuildYears': '',
            'minHouseHoldCount': '',
            'maxHouseHoldCount': '',
            'showArticle': 'false',
            'sameAddressGroup': 'true',
            'page': 1,
            'complexNo': '',
            'buildingNo': ''
        }
        
        logger.info(f"ë‹¨ì§€ ê²€ìƒ‰: cortarNo={cortarNo}, tradeType={trade_type}")
        data = self._safe_request(url, params)
        
        if data and 'complexList' in data:
            complexes = data['complexList']
            logger.info(f"ê²€ìƒ‰ëœ ë‹¨ì§€ ìˆ˜: {len(complexes)}")
            return complexes
        
        logger.warning("ë‹¨ì§€ ê²€ìƒ‰ ì‹¤íŒ¨")
        return []
    
    def get_complex_articles(self, complex_no: str, trade_type: str = "A1") -> List[Dict]:
        """
        íŠ¹ì • ë‹¨ì§€ì˜ ë§¤ë¬¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            complex_no: ë‹¨ì§€ ë²ˆí˜¸
            trade_type: ê±°ë˜ ìœ í˜•
            
        Returns:
            ë§¤ë¬¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        # API í˜¸ì¶œ ì „ ëœë”© í˜ì´ì§€ ë¨¼ì € ë°©ë¬¸ (ì¤‘ìš”!)
        self._visit_landing_page('complex')
        
        url = f"{self.BASE_URL}/api/articles/complex/{complex_no}"
        
        params = {
            'realEstateType': 'APT:OPST',
            'tradeType': trade_type,
            'tag': '::::::::',
            'rentPriceMin': 0,
            'rentPriceMax': 999999,
            'priceMin': 0,
            'priceMax': 999999,
            'areaMin': 0,
            'areaMax': 999999,
            'oldBuildYears': '',
            'recentlyBuildYears': '',
            'minHouseHoldCount': '',
            'maxHouseHoldCount': '',
            'showArticle': 'true',
            'sameAddressGroup': 'false',
            'minMoveInMonth': '',
            'maxMoveInMonth': '',
            'page': 1
        }
        
        logger.info(f"ë§¤ë¬¼ ê²€ìƒ‰: complexNo={complex_no}")
        data = self._safe_request(url, params)
        
        if data and 'articleList' in data:
            articles = data['articleList']
            logger.info(f"ê²€ìƒ‰ëœ ë§¤ë¬¼ ìˆ˜: {len(articles)}")
            
            # í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜
            self._simulate_scroll()
            
            # ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•œ ëŒ€ê¸° (2ë¶„~5ë¶„, ì •ê·œë¶„í¬)
            delay = self._human_like_delay(2.0, 5.0)  # 2-5ë¶„
            delay_minutes = delay / 60
            logger.info(f"ğŸ•’ ë§¤ë¬¼ ëª©ë¡ í™•ì¸ ì¤‘... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
            time.sleep(delay)
            
            return articles
        
        logger.warning(f"ë§¤ë¬¼ ê²€ìƒ‰ ì‹¤íŒ¨: complexNo={complex_no}")
        return []
    
    def get_article_detail(self, article_no: str) -> Optional[Dict]:
        """
        ë§¤ë¬¼ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            article_no: ë§¤ë¬¼ ë²ˆí˜¸
            
        Returns:
            ë§¤ë¬¼ ìƒì„¸ ì •ë³´
        """
        url = f"{self.BASE_URL}/api/articles/{article_no}"
        
        logger.info(f"ë§¤ë¬¼ ìƒì„¸ ì •ë³´: articleNo={article_no}")
        data = self._safe_request(url)
        
        # ì‚¬ëŒì²˜ëŸ¼ ìƒì„¸ ì •ë³´ ì½ê¸° (1ë¶„~3ë¶„, ì •ê·œë¶„í¬)
        delay = self._human_like_delay(1.0, 3.0)  # 1-3ë¶„
        delay_minutes = delay / 60
        logger.info(f"ğŸ“„ ìƒì„¸ ì •ë³´ ì½ëŠ” ì¤‘... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
        time.sleep(delay)
        
        return data
    
    def scrape_region(self, cortarNo: str, trade_types: List[str] = ["A1"]) -> List[Dict]:
        """
        íŠ¹ì • ì§€ì—­ì˜ ëª¨ë“  ë§¤ë¬¼ í¬ë¡¤ë§
        
        Args:
            cortarNo: ì§€ì—­ ì½”ë“œ
            trade_types: ê±°ë˜ ìœ í˜• ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ëª¨ë“  ë§¤ë¬¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        all_properties = []
        
        for idx, trade_type in enumerate(trade_types):
            logger.info(f"=== ê±°ë˜ ìœ í˜• {trade_type} í¬ë¡¤ë§ ì‹œì‘ ===")
            logger.info(f"ğŸ“Š ì§„í–‰ ìƒí™©: {idx + 1}/{len(trade_types)}, ì´ ìš”ì²­: {self.request_count}íšŒ, í”¼ë¡œë„: {self.fatigue_level:.2f}")
            
            # ê±°ë˜ ìœ í˜• ê°„ Long Sleep (2ë²ˆì§¸ë¶€í„°, 30ë¶„~60ë¶„)
            if idx > 0:
                long_delay = self._human_like_delay(30.0, 60.0)  # 30-60ë¶„
                long_delay_minutes = long_delay / 60
                logger.info(f"ğŸ”„ ê±°ë˜ ìœ í˜• ì „í™˜ íœ´ì‹: {long_delay_minutes:.1f}ë¶„ ({long_delay:.0f}ì´ˆ)")
                time.sleep(long_delay)
            
            # 1. ë‹¨ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            complexes = self.search_complexes(cortarNo, trade_type)
            
            # ìˆœì„œ ë¬´ì‘ìœ„í™” (Shuffle) - ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•˜ê²Œ!
            if complexes:
                random.shuffle(complexes)
                logger.info(f"ğŸ”€ ë‹¨ì§€ ìˆœì„œ ë¬´ì‘ìœ„í™” ì™„ë£Œ (ì´ {len(complexes)}ê°œ)")
            
            # 2. ê° ë‹¨ì§€ì˜ ë§¤ë¬¼ ê°€ì ¸ì˜¤ê¸°
            for i, complex_info in enumerate(complexes[:10], 1):  # í…ŒìŠ¤íŠ¸: ìƒìœ„ 10ê°œë§Œ
                complex_no = complex_info.get('complexNo')
                complex_name = complex_info.get('complexName', 'ì•Œ ìˆ˜ ì—†ìŒ')
                
                logger.info(f"[{i}/{len(complexes[:10])}] {complex_name} (complexNo: {complex_no})")
                
                # ë§ˆìš°ìŠ¤ í´ë¦­ ì „ ì›€ì§ì„ ì‹œë®¬ë ˆì´ì…˜
                self._simulate_mouse_movement()
                
                articles = self.get_complex_articles(complex_no, trade_type)
                
                # ë§¤ë¬¼ ìˆœì„œë„ ë¬´ì‘ìœ„í™” (Shuffle)
                if articles:
                    random.shuffle(articles)
                    logger.info(f"ğŸ”€ ë§¤ë¬¼ ìˆœì„œ ë¬´ì‘ìœ„í™” ì™„ë£Œ (ì´ {len(articles)}ê°œ)")
                
                for article in articles:
                    # ë§¤ë¬¼ ë°ì´í„° ê°€ê³µ
                    property_data = self._parse_article(article, complex_info, trade_type)
                    all_properties.append(property_data)
                
                # ë‹¨ì§€ ê°„ Long Sleep (5ë¶„~10ë¶„)
                delay = self._human_like_delay(5.0, 10.0)  # 5-10ë¶„
                delay_minutes = delay / 60
                logger.info(f"ğŸ¢ ë‹¤ìŒ ë‹¨ì§€ë¡œ ì´ë™... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
                time.sleep(delay)
                
                # ê°€ë” ì¶”ê°€ Long Sleep (20% í™•ë¥ ë¡œ 15~30ë¶„ íœ´ì‹)
                if random.random() < 0.2:
                    long_break_minutes = random.uniform(15, 30)
                    long_break_seconds = long_break_minutes * 60
                    logger.info(f"ğŸ’¤ ì¶”ê°€ ì¥ì‹œê°„ íœ´ì‹: {long_break_minutes:.1f}ë¶„ ({long_break_seconds:.0f}ì´ˆ)")
                    time.sleep(long_break_seconds)
        
        logger.info(f"ì´ {len(all_properties)}ê°œ ë§¤ë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")
        return all_properties
    
    def _parse_article(self, article: Dict, complex_info: Dict, trade_type: str) -> Dict:
        """
        ë§¤ë¬¼ ë°ì´í„° íŒŒì‹±
        
        Args:
            article: ë§¤ë¬¼ ì›ë³¸ ë°ì´í„°
            complex_info: ë‹¨ì§€ ì •ë³´
            trade_type: ê±°ë˜ ìœ í˜•
            
        Returns:
            íŒŒì‹±ëœ ë§¤ë¬¼ ì •ë³´
        """
        article_no = article.get('articleNo', '')
        complex_no = complex_info.get('complexNo', '')
        
        return {
            'id': f"{complex_no}_{article_no}",
            'complex_no': complex_no,
            'complex_name': complex_info.get('complexName', ''),
            'article_no': article_no,
            'price': article.get('dealOrWarrantPrc', 0),  # ë§¤ë§¤ê°€ ë˜ëŠ” ì „ì„¸ê°€
            'area_real': article.get('area1', 0),  # ê³µê¸‰ë©´ì 
            'area_exclusive': article.get('area2', 0),  # ì „ìš©ë©´ì 
            'floor': article.get('floorInfo', ''),
            'total_floors': complex_info.get('maxFloor', 0),
            'direction': article.get('direction', ''),
            'trade_type': trade_type,
            'approval_year': complex_info.get('useApproveYmd', '')[:4] if complex_info.get('useApproveYmd') else 0,
            'household_count': complex_info.get('totalHouseholdCount', 0),
            'room_count': article.get('roomCnt', 0),
            'bathroom_count': article.get('bathroomCnt', 0),
            'loan_amount': article.get('loanAmount', 0),
            'description': article.get('tagList', []),
            'url': f"https://new.land.naver.com/complexes/{complex_no}?articleNo={article_no}"
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    scraper = NaverRealEstateScraper()
    
    # ê°•ë‚¨êµ¬ ëŒ€ì¹˜ë™ ì§€ì—­ ì½”ë“œ
    cortarNo = "1168010600"
    
    properties = scraper.scrape_region(cortarNo, trade_types=["A1"])
    
    print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {len(properties)}ê°œ ë§¤ë¬¼")
    if properties:
        print("\nì²« ë²ˆì§¸ ë§¤ë¬¼ ì˜ˆì‹œ:")
        print(properties[0])
