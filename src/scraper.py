"""
ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ë§ ëª¨ë“ˆ
APIë¥¼ í†µí•´ ë§¤ë¬¼ ì •ë³´ ìˆ˜ì§‘
"""

import requests
import random
import time
from typing import List, Dict, Optional
import logging
from datetime import datetime
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NaverRealEstateScraper:
    """ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    # ë‹¤ì–‘í•œ User-Agent ë¦¬ìŠ¤íŠ¸ (ì°¨ë‹¨ íšŒí”¼) - ë” ë‹¤ì–‘í•˜ê²Œ!
    USER_AGENTS = [
        # Chrome (Windows)
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        # Chrome (Mac)
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        # Firefox (Windows)
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        # Firefox (Mac)
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0',
        # Edge
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',
        # Safari (Mac)
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
    ]
    
    BASE_URL = "https://new.land.naver.com"
    
    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self._update_headers()
        self._visit_homepage()  # ì´ˆê¸° ë°©ë¬¸ìœ¼ë¡œ ì¿ í‚¤ ë°›ê¸°
        
        # ì‚¬ëŒì²˜ëŸ¼ í–‰ë™í•˜ê¸° ìœ„í•œ ìƒíƒœ ê´€ë¦¬
        self.request_count = 0  # ì´ ìš”ì²­ íšŸìˆ˜
        self.last_break_count = 0  # ë§ˆì§€ë§‰ íœ´ì‹ ì‹œì 
        self.session_start_time = time.time()  # ì„¸ì…˜ ì‹œì‘ ì‹œê°„
        self.fatigue_level = 0.0  # í”¼ë¡œë„ (0.0 ~ 1.0)
    
    def _visit_homepage(self):
        """
        ë„¤ì´ë²„ ë¶€ë™ì‚° í™ˆí˜ì´ì§€ ë°©ë¬¸ (ì¿ í‚¤ ë°›ê¸°)
        ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë™ì‘í•˜ê¸° ìœ„í•´
        """
        try:
            logger.info("ë„¤ì´ë²„ ë¶€ë™ì‚° í™ˆí˜ì´ì§€ ë°©ë¬¸ ì¤‘...")
            self.session.get(self.BASE_URL, timeout=10)
            time.sleep(random.uniform(2, 4))
            logger.info("ì´ˆê¸° ë°©ë¬¸ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ì´ˆê¸° ë°©ë¬¸ ì‹¤íŒ¨: {e}")
    
    def _update_headers(self):
        """ìš”ì²­ í—¤ë” ì—…ë°ì´íŠ¸ (ì°¨ë‹¨ íšŒí”¼)"""
        self.session.headers.update({
            'User-Agent': random.choice(self.USER_AGENTS),
            'Referer': 'https://new.land.naver.com/',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
    
    def _human_like_delay(self, base_min_minutes: float = 1.0, base_max_minutes: float = 3.0) -> float:
        """
        ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•œ ëŒ€ê¸° ì‹œê°„ ìƒì„± (ë¶„ ë‹¨ìœ„, ì •ê·œë¶„í¬ ì‚¬ìš©)
        
        Args:
            base_min_minutes: ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ë¶„)
            base_max_minutes: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ë¶„)
            
        Returns:
            ì‹¤ì œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        # ì •ê·œë¶„í¬ë¡œ ë” ìì—°ìŠ¤ëŸ¬ìš´ ëœë¤ì„±
        mean = (base_min_minutes + base_max_minutes) / 2
        std = (base_max_minutes - base_min_minutes) / 4
        delay_minutes = np.random.normal(mean, std)
        
        # ìµœì†Œ/ìµœëŒ€ ë²”ìœ„ ë‚´ë¡œ ì œí•œ
        delay_minutes = max(base_min_minutes, min(base_max_minutes, delay_minutes))
        
        # í”¼ë¡œë„ ë°˜ì˜ (ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ëŠë ¤ì§)
        delay_minutes *= (1 + self.fatigue_level * 0.5)
        
        # í™œë™ ì‹œê°„ëŒ€ ë°˜ì˜ (ë‚® ì‹œê°„ vs ë°¤ ì‹œê°„)
        hour = datetime.now().hour
        if 9 <= hour <= 18:  # ì˜¤ì „ 9ì‹œ ~ ì˜¤í›„ 6ì‹œ (í™œë°œ)
            delay_minutes *= 0.8
        elif hour >= 22 or hour <= 6:  # ë°¤ 10ì‹œ ~ ìƒˆë²½ 6ì‹œ (ëŠë¦¼)
            delay_minutes *= 1.3
        
        # ë¶„ì„ ì´ˆë¡œ ë³€í™˜
        return delay_minutes * 60
    
    def _should_take_break(self) -> bool:
        """
        íœ´ì‹ì´ í•„ìš”í•œì§€ íŒë‹¨ (ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•˜ê²Œ)
        
        Returns:
            True if íœ´ì‹ í•„ìš”
        """
        requests_since_break = self.request_count - self.last_break_count
        
        # 5-10ê°œ ìš”ì²­ë§ˆë‹¤ íœ´ì‹ (ëœë¤)
        break_threshold = random.randint(5, 10)
        
        if requests_since_break >= break_threshold:
            # 80% í™•ë¥ ë¡œ íœ´ì‹ (ì™„ì „íˆ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ê²Œ)
            return random.random() < 0.8
        
        # ê°€ë” ê°‘ìê¸° íœ´ì‹ (5% í™•ë¥ )
        return random.random() < 0.05
    
    def _take_break(self):
        """
        ê¸´ íœ´ì‹ ì‹œê°„ (ì‚¬ëŒì´ ì»¤í”¼ ë§ˆì‹œê±°ë‚˜ ì ì‹¬ ë¨¹ëŠ” ì‹œê°„) - ë¶„ ë‹¨ìœ„
        """
        # ë² íƒ€ ë¶„í¬ë¡œ ë” ìì—°ìŠ¤ëŸ¬ìš´ íœ´ì‹ ì‹œê°„ (10ë¶„~30ë¶„, í‰ê·  20ë¶„)
        alpha, beta = 2, 2
        normalized = np.random.beta(alpha, beta)
        break_minutes = 10 + normalized * 20  # 10~30ë¶„
        break_seconds = break_minutes * 60
        
        self.last_break_count = self.request_count
        
        logger.info(f"â˜• ì¥ì‹œê°„ íœ´ì‹ (ì ì‹¬/ì»¤í”¼): {break_minutes:.1f}ë¶„ ({break_seconds:.0f}ì´ˆ) ëŒ€ê¸°...")
        logger.info(f"   (ì´ {self.request_count}ê°œ ìš”ì²­ ì™„ë£Œ, í”¼ë¡œë„: {self.fatigue_level:.2f})")
        
        time.sleep(break_seconds)
        
        # íœ´ì‹ í›„ í”¼ë¡œë„ ê°ì†Œ
        self.fatigue_level = max(0, self.fatigue_level - 0.2)
    
    def _simulate_reading(self):
        """
        í˜ì´ì§€ë¥¼ ì½ëŠ” ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ìŠ¤í¬ë¡¤, í´ë¦­ ë“±) - ë¶„ ë‹¨ìœ„
        """
        # ê°ë§ˆ ë¶„í¬ë¡œ ì½ê¸° ì‹œê°„ (1ë¶„~5ë¶„, í‰ê·  2.5ë¶„)
        reading_minutes = np.random.gamma(2, 1.5)
        reading_minutes = min(5, max(1, reading_minutes))
        reading_seconds = reading_minutes * 60
        
        logger.info(f"ğŸ“– ë§¤ë¬¼ ìƒì„¸ ì½ëŠ” ì¤‘... {reading_minutes:.1f}ë¶„ ({reading_seconds:.0f}ì´ˆ)")
        time.sleep(reading_seconds)
    
    def _update_fatigue(self):
        """
        í”¼ë¡œë„ ì—…ë°ì´íŠ¸ (ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ì¦ê°€)
        """
        session_duration = (time.time() - self.session_start_time) / 3600  # ì‹œê°„ ë‹¨ìœ„
        self.fatigue_level = min(1.0, session_duration * 0.1)  # 10ì‹œê°„ í›„ ìµœëŒ€
    
    def _safe_request(self, url: str, params: Dict = None, retry: int = 3) -> Optional[Dict]:
        """
        ì•ˆì „í•œ HTTP ìš”ì²­ (ì¬ì‹œë„ í¬í•¨, 429 ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬, ì‚¬ëŒì²˜ëŸ¼ í–‰ë™)
        
        Args:
            url: ìš”ì²­ URL
            params: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
            retry: ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            JSON ì‘ë‹µ ë˜ëŠ” None
        """
        # ìš”ì²­ ì „ íœ´ì‹ í•„ìš” ì—¬ë¶€ í™•ì¸
        if self._should_take_break():
            self._take_break()
        
        for attempt in range(retry):
            try:
                # ìš”ì²­ë§ˆë‹¤ User-Agent ë³€ê²½ (ë‹¤ì–‘í•œ ë¸Œë¼ìš°ì € ì‚¬ìš©)
                self._update_headers()
                
                # ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•œ ëŒ€ê¸° (ë¶„ ë‹¨ìœ„, ì •ê·œë¶„í¬)
                if attempt == 0:
                    delay = self._human_like_delay(1.0, 3.0)  # 1-3ë¶„
                    delay_minutes = delay / 60
                    logger.info(f"ğŸ¤” ìƒê°í•˜ëŠ” ì¤‘... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
                    time.sleep(delay)
                
                # ìš”ì²­ ì¹´ìš´íŠ¸ ì¦ê°€ ë° í”¼ë¡œë„ ì—…ë°ì´íŠ¸
                self.request_count += 1
                self._update_fatigue()
                
                response = self.session.get(url, params=params, timeout=30)
                
                if response.status_code == 200:
                    # ì„±ê³µ ì‹œ í˜ì´ì§€ ì½ê¸° ì‹œë®¬ë ˆì´ì…˜ (30% í™•ë¥ )
                    if random.random() < 0.3:
                        self._simulate_reading()
                    return response.json()
                
                elif response.status_code == 429:
                    # 429 Too Many Requests - 30ë¶„ ëŒ€ê¸°!
                    wait_minutes = 30
                    wait_seconds = wait_minutes * 60
                    
                    logger.warning(f"âš ï¸  429 ì—ëŸ¬ (Too Many Requests) ë°œìƒ!")
                    logger.info(f"ğŸš¨ í¬ë¡¤ë§ ì°¨ë‹¨ ê°ì§€ - {wait_minutes}ë¶„ ({wait_seconds}ì´ˆ) ëŒ€ê¸°...")
                    logger.info(f"   í˜„ì¬ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    logger.info(f"   ì¬ê°œ ì˜ˆì •: {(datetime.now().timestamp() + wait_seconds)}")
                    
                    if attempt < retry - 1:
                        logger.info(f"ğŸ• {wait_minutes}ë¶„ íœ´ì‹ í›„ ì¬ì‹œë„ ì˜ˆì • ({attempt + 2}/{retry})")
                        time.sleep(wait_seconds)
                        self._update_headers()  # User-Agent ë³€ê²½
                        logger.info("âœ… íœ´ì‹ ì™„ë£Œ. í¬ë¡¤ë§ ì¬ê°œ...")
                    else:
                        logger.error("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. í”„ë¡œê·¸ë¨ì„ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
                        return None
                
                elif response.status_code == 403:
                    # 403 Forbidden
                    logger.warning(f"ì ‘ê·¼ ê±°ë¶€ (403). í—¤ë” ë³€ê²½ í›„ ì¬ì‹œë„... ({attempt + 1}/{retry})")
                    self._update_headers()
                    delay = random.uniform(5, 10)
                    time.sleep(delay)
                
                else:
                    logger.warning(f"ì‘ë‹µ ì½”ë“œ {response.status_code}")
                    if attempt < retry - 1:
                        delay = random.uniform(3, 7)
                        time.sleep(delay)
                
            except requests.exceptions.RequestException as e:
                logger.error(f"ìš”ì²­ ì˜¤ë¥˜: {e}")
                if attempt < retry - 1:
                    delay = random.uniform(5, 10)
                    logger.info(f"ì˜¤ë¥˜ í›„ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(delay)
        
        return None
    
    def search_complexes(self, cortarNo: str, trade_type: str = "A1") -> List[Dict]:
        """
        ì§€ì—­ë³„ ë‹¨ì§€ ê²€ìƒ‰
        
        Args:
            cortarNo: ì§€ì—­ ì½”ë“œ (ì˜ˆ: 1168010600 - ê°•ë‚¨êµ¬ ëŒ€ì¹˜ë™)
            trade_type: ê±°ë˜ ìœ í˜• (A1: ë§¤ë§¤, B1: ì „ì„¸, B2: ì›”ì„¸, B3: ë‹¨ê¸°ì„ëŒ€)
            
        Returns:
            ë‹¨ì§€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        url = f"{self.BASE_URL}/api/complexes"
        
        params = {
            'cortarNo': cortarNo,
            'realEstateType': 'APT:OPST',  # ì•„íŒŒíŠ¸, ì˜¤í”¼ìŠ¤í…”
            'tradeType': trade_type,
            'tag': '::::::::',
            'rentPriceMin': 0,
            'rentPriceMax': 999999,
            'priceMin': 0,
            'priceMax': 999999,
            'areaMin': 0,
            'areaMax': 999999,
            'oldBuildYears': '',
            'recentlyBuildYears': '',
            'minHouseHoldCount': '',
            'maxHouseHoldCount': '',
            'showArticle': 'false',
            'sameAddressGroup': 'true',
            'page': 1,
            'complexNo': '',
            'buildingNo': ''
        }
        
        logger.info(f"ë‹¨ì§€ ê²€ìƒ‰: cortarNo={cortarNo}, tradeType={trade_type}")
        data = self._safe_request(url, params)
        
        if data and 'complexList' in data:
            complexes = data['complexList']
            logger.info(f"ê²€ìƒ‰ëœ ë‹¨ì§€ ìˆ˜: {len(complexes)}")
            return complexes
        
        logger.warning("ë‹¨ì§€ ê²€ìƒ‰ ì‹¤íŒ¨")
        return []
    
    def get_complex_articles(self, complex_no: str, trade_type: str = "A1") -> List[Dict]:
        """
        íŠ¹ì • ë‹¨ì§€ì˜ ë§¤ë¬¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            complex_no: ë‹¨ì§€ ë²ˆí˜¸
            trade_type: ê±°ë˜ ìœ í˜•
            
        Returns:
            ë§¤ë¬¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        url = f"{self.BASE_URL}/api/articles/complex/{complex_no}"
        
        params = {
            'realEstateType': 'APT:OPST',
            'tradeType': trade_type,
            'tag': '::::::::',
            'rentPriceMin': 0,
            'rentPriceMax': 999999,
            'priceMin': 0,
            'priceMax': 999999,
            'areaMin': 0,
            'areaMax': 999999,
            'oldBuildYears': '',
            'recentlyBuildYears': '',
            'minHouseHoldCount': '',
            'maxHouseHoldCount': '',
            'showArticle': 'true',
            'sameAddressGroup': 'false',
            'minMoveInMonth': '',
            'maxMoveInMonth': '',
            'page': 1
        }
        
        logger.info(f"ë§¤ë¬¼ ê²€ìƒ‰: complexNo={complex_no}")
        data = self._safe_request(url, params)
        
        if data and 'articleList' in data:
            articles = data['articleList']
            logger.info(f"ê²€ìƒ‰ëœ ë§¤ë¬¼ ìˆ˜: {len(articles)}")
            
            # ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•œ ëŒ€ê¸° (2ë¶„~5ë¶„, ì •ê·œë¶„í¬)
            delay = self._human_like_delay(2.0, 5.0)  # 2-5ë¶„
            delay_minutes = delay / 60
            logger.info(f"ğŸ•’ ë§¤ë¬¼ ëª©ë¡ í™•ì¸ ì¤‘... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
            time.sleep(delay)
            
            return articles
        
        logger.warning(f"ë§¤ë¬¼ ê²€ìƒ‰ ì‹¤íŒ¨: complexNo={complex_no}")
        return []
    
    def get_article_detail(self, article_no: str) -> Optional[Dict]:
        """
        ë§¤ë¬¼ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            article_no: ë§¤ë¬¼ ë²ˆí˜¸
            
        Returns:
            ë§¤ë¬¼ ìƒì„¸ ì •ë³´
        """
        url = f"{self.BASE_URL}/api/articles/{article_no}"
        
        logger.info(f"ë§¤ë¬¼ ìƒì„¸ ì •ë³´: articleNo={article_no}")
        data = self._safe_request(url)
        
        # ì‚¬ëŒì²˜ëŸ¼ ìƒì„¸ ì •ë³´ ì½ê¸° (1ë¶„~3ë¶„, ì •ê·œë¶„í¬)
        delay = self._human_like_delay(1.0, 3.0)  # 1-3ë¶„
        delay_minutes = delay / 60
        logger.info(f"ğŸ“„ ìƒì„¸ ì •ë³´ ì½ëŠ” ì¤‘... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
        time.sleep(delay)
        
        return data
    
    def scrape_region(self, cortarNo: str, trade_types: List[str] = ["A1"]) -> List[Dict]:
        """
        íŠ¹ì • ì§€ì—­ì˜ ëª¨ë“  ë§¤ë¬¼ í¬ë¡¤ë§
        
        Args:
            cortarNo: ì§€ì—­ ì½”ë“œ
            trade_types: ê±°ë˜ ìœ í˜• ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ëª¨ë“  ë§¤ë¬¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        all_properties = []
        
        for idx, trade_type in enumerate(trade_types):
            logger.info(f"=== ê±°ë˜ ìœ í˜• {trade_type} í¬ë¡¤ë§ ì‹œì‘ ===")
            logger.info(f"ğŸ“Š ì§„í–‰ ìƒí™©: {idx + 1}/{len(trade_types)}, ì´ ìš”ì²­: {self.request_count}íšŒ, í”¼ë¡œë„: {self.fatigue_level:.2f}")
            
            # ê±°ë˜ ìœ í˜• ê°„ Long Sleep (2ë²ˆì§¸ë¶€í„°, 30ë¶„~60ë¶„)
            if idx > 0:
                long_delay = self._human_like_delay(30.0, 60.0)  # 30-60ë¶„
                long_delay_minutes = long_delay / 60
                logger.info(f"ğŸ”„ ê±°ë˜ ìœ í˜• ì „í™˜ íœ´ì‹: {long_delay_minutes:.1f}ë¶„ ({long_delay:.0f}ì´ˆ)")
                time.sleep(long_delay)
            
            # 1. ë‹¨ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            complexes = self.search_complexes(cortarNo, trade_type)
            
            # 2. ê° ë‹¨ì§€ì˜ ë§¤ë¬¼ ê°€ì ¸ì˜¤ê¸°
            for i, complex_info in enumerate(complexes[:10], 1):  # í…ŒìŠ¤íŠ¸: ìƒìœ„ 10ê°œë§Œ
                complex_no = complex_info.get('complexNo')
                complex_name = complex_info.get('complexName', 'ì•Œ ìˆ˜ ì—†ìŒ')
                
                logger.info(f"[{i}/{len(complexes[:10])}] {complex_name} (complexNo: {complex_no})")
                
                articles = self.get_complex_articles(complex_no, trade_type)
                
                for article in articles:
                    # ë§¤ë¬¼ ë°ì´í„° ê°€ê³µ
                    property_data = self._parse_article(article, complex_info, trade_type)
                    all_properties.append(property_data)
                
                # ë‹¨ì§€ ê°„ Long Sleep (5ë¶„~10ë¶„)
                delay = self._human_like_delay(5.0, 10.0)  # 5-10ë¶„
                delay_minutes = delay / 60
                logger.info(f"ğŸ¢ ë‹¤ìŒ ë‹¨ì§€ë¡œ ì´ë™... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
                time.sleep(delay)
                
                # ê°€ë” ì¶”ê°€ Long Sleep (20% í™•ë¥ ë¡œ 15~30ë¶„ íœ´ì‹)
                if random.random() < 0.2:
                    long_break_minutes = random.uniform(15, 30)
                    long_break_seconds = long_break_minutes * 60
                    logger.info(f"ğŸ’¤ ì¶”ê°€ ì¥ì‹œê°„ íœ´ì‹: {long_break_minutes:.1f}ë¶„ ({long_break_seconds:.0f}ì´ˆ)")
                    time.sleep(long_break_seconds)
        
        logger.info(f"ì´ {len(all_properties)}ê°œ ë§¤ë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")
        return all_properties
    
    def _parse_article(self, article: Dict, complex_info: Dict, trade_type: str) -> Dict:
        """
        ë§¤ë¬¼ ë°ì´í„° íŒŒì‹±
        
        Args:
            article: ë§¤ë¬¼ ì›ë³¸ ë°ì´í„°
            complex_info: ë‹¨ì§€ ì •ë³´
            trade_type: ê±°ë˜ ìœ í˜•
            
        Returns:
            íŒŒì‹±ëœ ë§¤ë¬¼ ì •ë³´
        """
        article_no = article.get('articleNo', '')
        complex_no = complex_info.get('complexNo', '')
        
        return {
            'id': f"{complex_no}_{article_no}",
            'complex_no': complex_no,
            'complex_name': complex_info.get('complexName', ''),
            'article_no': article_no,
            'price': article.get('dealOrWarrantPrc', 0),  # ë§¤ë§¤ê°€ ë˜ëŠ” ì „ì„¸ê°€
            'area_real': article.get('area1', 0),  # ê³µê¸‰ë©´ì 
            'area_exclusive': article.get('area2', 0),  # ì „ìš©ë©´ì 
            'floor': article.get('floorInfo', ''),
            'total_floors': complex_info.get('maxFloor', 0),
            'direction': article.get('direction', ''),
            'trade_type': trade_type,
            'approval_year': complex_info.get('useApproveYmd', '')[:4] if complex_info.get('useApproveYmd') else 0,
            'household_count': complex_info.get('totalHouseholdCount', 0),
            'room_count': article.get('roomCnt', 0),
            'bathroom_count': article.get('bathroomCnt', 0),
            'loan_amount': article.get('loanAmount', 0),
            'description': article.get('tagList', []),
            'url': f"https://new.land.naver.com/complexes/{complex_no}?articleNo={article_no}"
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    scraper = NaverRealEstateScraper()
    
    # ê°•ë‚¨êµ¬ ëŒ€ì¹˜ë™ ì§€ì—­ ì½”ë“œ
    cortarNo = "1168010600"
    
    properties = scraper.scrape_region(cortarNo, trade_types=["A1"])
    
    print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {len(properties)}ê°œ ë§¤ë¬¼")
    if properties:
        print("\nì²« ë²ˆì§¸ ë§¤ë¬¼ ì˜ˆì‹œ:")
        print(properties[0])
