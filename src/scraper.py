"""
ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ë§ ëª¨ë“ˆ
APIë¥¼ í†µí•´ ë§¤ë¬¼ ì •ë³´ ìˆ˜ì§‘

âœ… Selenium + requests í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹:
- Selenium: ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸, ì¿ í‚¤ íšë“ (ì‹¤ì œ ë¸Œë¼ìš°ì €)
- requests: API í˜¸ì¶œ (ë¹ ë¦„)
"""

import requests
import random
import time
from typing import List, Dict, Optional
import logging
from datetime import datetime
import numpy as np

# Selenium ê´€ë ¨ ì„í¬íŠ¸
try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    logging.warning("Seleniumì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install undetected-chromedriver ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NaverRealEstateScraper:
    """ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    # ë¸Œë¼ìš°ì €ë³„ ì™„ì „í•œ í”„ë¡œíŒŒì¼ (Fingerprinting ìš°íšŒ)
    BROWSER_PROFILES = [
        # Chrome 121 (Windows) - ìµœì‹  ë²„ì „
        {
            'type': 'chrome',
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'sec_ch_ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'sec_ch_ua_mobile': '?0',
            'sec_ch_ua_platform': '"Windows"',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Chrome 120 (Windows)
        {
            'type': 'chrome',
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'sec_ch_ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec_ch_ua_mobile': '?0',
            'sec_ch_ua_platform': '"Windows"',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Chrome 121 (Mac)
        {
            'type': 'chrome',
            'user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'sec_ch_ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'sec_ch_ua_mobile': '?0',
            'sec_ch_ua_platform': '"macOS"',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Firefox 122 (Windows)
        {
            'type': 'firefox',
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Firefox 121 (Windows)
        {
            'type': 'firefox',
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Firefox 122 (Mac)
        {
            'type': 'firefox',
            'user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Edge 121 (Windows)
        {
            'type': 'edge',
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',
            'sec_ch_ua': '"Not A(Brand";v="99", "Microsoft Edge";v="121", "Chromium";v="121"',
            'sec_ch_ua_mobile': '?0',
            'sec_ch_ua_platform': '"Windows"',
            'accept': 'application/json, text/plain, */*',
            'accept_language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        },
        # Safari 17.2 (Mac)
        {
            'type': 'safari',
            'user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
            'accept': 'application/json, text/javascript, */*; q=0.01',
            'accept_language': 'ko-KR,ko;q=0.9',
        },
    ]
    
    BASE_URL = "https://new.land.naver.com"
    
    def __init__(self, use_selenium: bool = True):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            use_selenium: Selenium ì‚¬ìš© ì—¬ë¶€ (True: ì‹¤ì œ ë¸Œë¼ìš°ì €, False: requestsë§Œ)
        """
        self.session = requests.Session()
        self.use_selenium = use_selenium and SELENIUM_AVAILABLE
        self.driver = None  # Selenium WebDriver
        
        # âœ… ê°œì„ : ì„¸ì…˜ ì‹œì‘ ì‹œ ë¸Œë¼ìš°ì € í”„ë¡œí•„ì„ í•œ ë²ˆë§Œ ì„ íƒ (í•µì‹¬!)
        # ì‹¤ì œ ì‚¬ìš©ìëŠ” í•œ ì„¸ì…˜ì—ì„œ ë¸Œë¼ìš°ì €ë¥¼ ë°”ê¾¸ì§€ ì•ŠìŒ!
        self.browser_profile = random.choice(self.BROWSER_PROFILES)
        
        self.cookies_received = False  # ì¿ í‚¤ ìˆ˜ì‹  ì—¬ë¶€
        self.last_cookie_refresh = time.time()  # ë§ˆì§€ë§‰ ì¿ í‚¤ ê°±ì‹  ì‹œê°„
        
        self._set_fixed_headers()  # í—¤ë”ë¥¼ í•œ ë²ˆë§Œ ì„¤ì •
        
        # Selenium ì´ˆê¸°í™”
        if self.use_selenium:
            self._init_selenium()
        
        self._visit_homepage()  # ì´ˆê¸° ë°©ë¬¸ìœ¼ë¡œ ì¿ í‚¤ ë°›ê¸°
        
        # ì‚¬ëŒì²˜ëŸ¼ í–‰ë™í•˜ê¸° ìœ„í•œ ìƒíƒœ ê´€ë¦¬
        self.request_count = 0  # ì´ ìš”ì²­ íšŸìˆ˜
        self.last_break_count = 0  # ë§ˆì§€ë§‰ íœ´ì‹ ì‹œì 
        self.session_start_time = time.time()  # ì„¸ì…˜ ì‹œì‘ ì‹œê°„
        self.fatigue_level = 0.0  # í”¼ë¡œë„ (0.0 ~ 1.0)
    
    def _init_selenium(self):
        """
        âœ… Selenium WebDriver ì´ˆê¸°í™” (undetected-chromedriver)
        
        ì‹¤ì œ Chrome ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿ í‚¤ë¥¼ íšë“í•©ë‹ˆë‹¤.
        """
        try:
            logger.info("ğŸš€ Selenium (Chrome) ì´ˆê¸°í™” ì¤‘...")
            
            # undetected-chromedriver ì˜µì…˜ ì„¤ì •
            options = uc.ChromeOptions()
            
            # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
            # options.add_argument('--headless')  # ë””ë²„ê¹… ì‹œ ì£¼ì„ ì²˜ë¦¬
            
            # ê¸°íƒ€ ì˜µì…˜
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument(f'user-agent={self.browser_profile["user_agent"]}')
            
            # WebDriver ìƒì„±
            self.driver = uc.Chrome(options=options, version_main=None)
            
            logger.info("âœ… Selenium ì´ˆê¸°í™” ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ Selenium ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.warning("âš ï¸  requests ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self.use_selenium = False
            self.driver = None
    
    def _visit_homepage(self):
        """
        âœ… ë„¤ì´ë²„ ë¶€ë™ì‚° í™ˆí˜ì´ì§€ ë°©ë¬¸ (ì¿ í‚¤ ë°›ê¸°)
        
        Selenium ì‚¬ìš© ì‹œ: ì‹¤ì œ ë¸Œë¼ìš°ì €ë¡œ ë°©ë¬¸í•˜ì—¬ JavaScript ì‹¤í–‰ â†’ ì¿ í‚¤ íšë“!
        requests ì‚¬ìš© ì‹œ: ê¸°ì¡´ ë°©ì‹ (ì¿ í‚¤ íšë“ ì‹¤íŒ¨ ê°€ëŠ¥)
        """
        if self.use_selenium and self.driver:
            # âœ… Seleniumìœ¼ë¡œ ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ (ì¿ í‚¤ íšë“ ì„±ê³µ!)
            try:
                logger.info("ğŸŒ Seleniumìœ¼ë¡œ ë„¤ì´ë²„ ë¶€ë™ì‚° ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ ì¤‘...")
                
                self.driver.get(self.BASE_URL)
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                # ì¿ í‚¤ íšë“ ë° requests Sessionì— ì „ë‹¬
                selenium_cookies = self.driver.get_cookies()
                
                if selenium_cookies:
                    for cookie in selenium_cookies:
                        self.session.cookies.set(cookie['name'], cookie['value'])
                    
                    self.cookies_received = True
                    self.last_cookie_refresh = time.time()
                    logger.info(f"âœ… ì¿ í‚¤ ìˆ˜ì‹  ì„±ê³µ: {len(selenium_cookies)}ê°œ")
                    
                    # ì£¼ìš” ì¿ í‚¤ ë¡œê¹…
                    cookie_names = [c['name'] for c in selenium_cookies]
                    important_cookies = ['NNB', 'JSESSIONID', 'nid_inf', 'NID_AUT', 'NID_SES']
                    found_cookies = [key for key in important_cookies if key in cookie_names]
                    if found_cookies:
                        logger.info(f"ğŸª ì£¼ìš” ì¿ í‚¤ í™•ì¸: {', '.join(found_cookies)}")
                else:
                    logger.warning("âš ï¸  ì¿ í‚¤ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
                time.sleep(random.uniform(2, 4))
                logger.info("âœ… Selenium ì´ˆê¸° ë°©ë¬¸ ì™„ë£Œ (ì„¸ì…˜ ì¤€ë¹„ë¨)")
                
            except Exception as e:
                logger.error(f"âŒ Selenium ë°©ë¬¸ ì‹¤íŒ¨: {e}")
                logger.warning("âš ï¸  requests ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.use_selenium = False
        
        else:
            # âŒ requestsë§Œ ì‚¬ìš© (ì¿ í‚¤ íšë“ ì‹¤íŒ¨ ê°€ëŠ¥)
            try:
                logger.info("ğŸŒ ë„¤ì´ë²„ ë¶€ë™ì‚° ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ ì¤‘ (requests)...")
                
                # Accept í—¤ë”ë¥¼ HTML í˜ì´ì§€ìš©ìœ¼ë¡œ ë³€ê²½
                original_accept = self.session.headers.get('Accept', '')
                self.session.headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'
                
                # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
                response = self.session.get(self.BASE_URL, timeout=10)
                
                # ì¿ í‚¤ ìˆ˜ì‹  í™•ì¸
                cookies = self.session.cookies.get_dict()
                if cookies:
                    self.cookies_received = True
                    self.last_cookie_refresh = time.time()
                    logger.info(f"âœ… ì¿ í‚¤ ìˆ˜ì‹  ì„±ê³µ: {len(cookies)}ê°œ")
                else:
                    logger.warning("âš ï¸  ì¿ í‚¤ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì°¨ë‹¨ë  ê°€ëŠ¥ì„± ë†’ìŒ!")
                
                # Accept í—¤ë” ë³µì›
                if original_accept:
                    self.session.headers['Accept'] = original_accept
                
                time.sleep(random.uniform(2, 4))
                logger.info("âœ… ì´ˆê¸° ë°©ë¬¸ ì™„ë£Œ (ì„¸ì…˜ ì¤€ë¹„ë¨)")
                
            except Exception as e:
                logger.warning(f"âŒ ì´ˆê¸° ë°©ë¬¸ ì‹¤íŒ¨: {e}")
    
    def _set_fixed_headers(self):
        """
        âœ… ê°œì„ : ì„¸ì…˜ ì‹œì‘ ì‹œ ë¸Œë¼ìš°ì € ì •ë³´ë¥¼ í•œ ë²ˆë§Œ ì„¤ì • (í•µì‹¬!)
        
        ì‹¤ì œ ì‚¬ìš©ìëŠ” í•œ ì„¸ì…˜ ë‚´ì—ì„œ ë¸Œë¼ìš°ì €ë¥¼ ë°”ê¾¸ì§€ ì•ŠìŠµë‹ˆë‹¤.
        ë™ì¼í•œ ì¿ í‚¤ë¥¼ ê°€ì§„ ìœ ì €ê°€ ë§¤ ìš”ì²­ë§ˆë‹¤ ë¸Œë¼ìš°ì €ë¥¼ ë°”ê¾¸ë©´
        ì„œë²„ëŠ” ì¦‰ì‹œ ë´‡ìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤!
        """
        profile = self.browser_profile
        browser_type = profile['type']
        
        # ê¸°ë³¸ í—¤ë” (ëª¨ë“  ë¸Œë¼ìš°ì € ê³µí†µ)
        headers = {
            'Host': 'new.land.naver.com',  # ëª…ì‹œì  ì„¤ì • (ì¤‘ìš”!)
            'User-Agent': profile['user_agent'],
            'Accept': profile['accept'],
            'Accept-Language': profile['accept_language'],
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://new.land.naver.com/',
            'Origin': 'https://new.land.naver.com',
            'Connection': 'keep-alive',
            'DNT': '1',  # Do Not Track
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }
        
        # Chrome/Edge ì „ìš© í—¤ë” (Sec-Fetch-*, sec-ch-ua)
        if browser_type in ['chrome', 'edge']:
            headers.update({
                'sec-ch-ua': profile['sec_ch_ua'],
                'sec-ch-ua-mobile': profile['sec_ch_ua_mobile'],
                'sec-ch-ua-platform': profile['sec_ch_ua_platform'],
                'Sec-Fetch-Site': 'same-origin',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Dest': 'empty',
            })
        
        # Firefox ì „ìš© í—¤ë”
        elif browser_type == 'firefox':
            headers.update({
                'TE': 'trailers',  # Firefox ê³ ìœ 
            })
        
        # Safari ì „ìš© í—¤ë”
        elif browser_type == 'safari':
            # SafariëŠ” sec-ch-ua ì—†ìŒ
            pass
        
        self.session.headers.clear()
        self.session.headers.update(headers)
        
        logger.info(f"ğŸŒ ë¸Œë¼ìš°ì € í”„ë¡œíŒŒì¼ ê³ ì •: {browser_type.upper()}")
        logger.info(f"   User-Agent: {profile['user_agent'][:80]}...")
    
    def _human_like_delay(self, base_min_minutes: float = 0.5, base_max_minutes: float = 1.5) -> float:
        """
        âœ… ê°œì„ : 2ë°° ë¹ ë¥¸ ì†ë„ë¡œ ì¡°ì • (1-3ë¶„ â†’ 0.5-1.5ë¶„)
        
        ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•œ ëŒ€ê¸° ì‹œê°„ ìƒì„± (ì •ê·œë¶„í¬ ì‚¬ìš©)
        
        Args:
            base_min_minutes: ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ë¶„)
            base_max_minutes: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ë¶„)
            
        Returns:
            ì‹¤ì œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        # ì •ê·œë¶„í¬ë¡œ ë” ìì—°ìŠ¤ëŸ¬ìš´ ëœë¤ì„±
        mean = (base_min_minutes + base_max_minutes) / 2
        std = (base_max_minutes - base_min_minutes) / 4
        delay_minutes = np.random.normal(mean, std)
        
        # ìµœì†Œ/ìµœëŒ€ ë²”ìœ„ ë‚´ë¡œ ì œí•œ
        delay_minutes = max(base_min_minutes, min(base_max_minutes, delay_minutes))
        
        # í”¼ë¡œë„ ë°˜ì˜ (ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ëŠë ¤ì§)
        delay_minutes *= (1 + self.fatigue_level * 0.5)
        
        # í™œë™ ì‹œê°„ëŒ€ ë°˜ì˜ (ë‚® ì‹œê°„ vs ë°¤ ì‹œê°„)
        hour = datetime.now().hour
        if 9 <= hour <= 18:  # ì˜¤ì „ 9ì‹œ ~ ì˜¤í›„ 6ì‹œ (í™œë°œ)
            delay_minutes *= 0.8
        elif hour >= 22 or hour <= 6:  # ë°¤ 10ì‹œ ~ ìƒˆë²½ 6ì‹œ (ëŠë¦¼)
            delay_minutes *= 1.3
        
        # ë¶„ì„ ì´ˆë¡œ ë³€í™˜
        return delay_minutes * 60
    
    def _should_take_break(self) -> bool:
        """
        íœ´ì‹ì´ í•„ìš”í•œì§€ íŒë‹¨ (ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•˜ê²Œ)
        
        Returns:
            True if íœ´ì‹ í•„ìš”
        """
        requests_since_break = self.request_count - self.last_break_count
        
        # 5-10ê°œ ìš”ì²­ë§ˆë‹¤ íœ´ì‹ (ëœë¤)
        break_threshold = random.randint(5, 10)
        
        if requests_since_break >= break_threshold:
            # 80% í™•ë¥ ë¡œ íœ´ì‹ (ì™„ì „íˆ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ê²Œ)
            return random.random() < 0.8
        
        # ê°€ë” ê°‘ìê¸° íœ´ì‹ (5% í™•ë¥ )
        return random.random() < 0.05
    
    def _take_break(self):
        """
        âœ… ê°œì„ : 2ë°° ë¹ ë¥¸ íœ´ì‹ ì‹œê°„ (10-30ë¶„ â†’ 5-15ë¶„)
        
        ê¸´ íœ´ì‹ ì‹œê°„ (ì‚¬ëŒì´ ì»¤í”¼ ë§ˆì‹œê±°ë‚˜ ì ì‹¬ ë¨¹ëŠ” ì‹œê°„)
        """
        # ë² íƒ€ ë¶„í¬ë¡œ ë” ìì—°ìŠ¤ëŸ¬ìš´ íœ´ì‹ ì‹œê°„ (5ë¶„~15ë¶„, í‰ê·  10ë¶„)
        alpha, beta = 2, 2
        normalized = np.random.beta(alpha, beta)
        break_minutes = 5 + normalized * 10  # 5~15ë¶„ (2ë°° ë¹ ë¦„!)
        break_seconds = break_minutes * 60
        
        self.last_break_count = self.request_count
        
        logger.info(f"â˜• ì¥ì‹œê°„ íœ´ì‹ (ì»¤í”¼/ê°„ì‹): {break_minutes:.1f}ë¶„ ({break_seconds:.0f}ì´ˆ) ëŒ€ê¸°...")
        logger.info(f"   (ì´ {self.request_count}ê°œ ìš”ì²­ ì™„ë£Œ, í”¼ë¡œë„: {self.fatigue_level:.2f})")
        
        time.sleep(break_seconds)
        
        # íœ´ì‹ í›„ í”¼ë¡œë„ ê°ì†Œ
        self.fatigue_level = max(0, self.fatigue_level - 0.2)
    
    def _simulate_reading(self):
        """
        âœ… ê°œì„ : requestsì—ì„œëŠ” ë§ˆìš°ìŠ¤/ìŠ¤í¬ë¡¤ì´ ì˜ë¯¸ ì—†ìœ¼ë¯€ë¡œ ë‹¨ìˆœ ì§€ì—°ìœ¼ë¡œ ë³€ê²½
        
        í˜ì´ì§€ë¥¼ ì½ëŠ” ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ì´ˆ ë‹¨ìœ„ë¡œ 2ë°° ë¹ ë¥´ê²Œ!)
        """
        # ê°ë§ˆ ë¶„í¬ë¡œ ì½ê¸° ì‹œê°„ (30ì´ˆ~150ì´ˆ, í‰ê·  75ì´ˆ)
        # ê¸°ì¡´: 1-5ë¶„ â†’ ê°œì„ : 0.5-2.5ë¶„ (2ë°° ë¹ ë¦„!)
        reading_seconds = np.random.gamma(2, 1.5) * 30
        reading_seconds = min(150, max(30, reading_seconds))
        reading_minutes = reading_seconds / 60
        
        logger.info(f"ğŸ“– í˜ì´ì§€ ì½ëŠ” ì¤‘... {reading_minutes:.1f}ë¶„ ({reading_seconds:.0f}ì´ˆ)")
        time.sleep(reading_seconds)
    
    def _update_fatigue(self):
        """
        í”¼ë¡œë„ ì—…ë°ì´íŠ¸ (ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ì¦ê°€)
        """
        session_duration = (time.time() - self.session_start_time) / 3600  # ì‹œê°„ ë‹¨ìœ„
        self.fatigue_level = min(1.0, session_duration * 0.1)  # 10ì‹œê°„ í›„ ìµœëŒ€
    
    def _check_and_refresh_cookies(self):
        """
        ì¿ í‚¤ ìœ íš¨ì„± ê²€ì‚¬ ë° í•„ìš”ì‹œ ì¬ë°©ë¬¸
        
        ë„¤ì´ë²„ ì¿ í‚¤ëŠ” ì‹œê°„ì´ ì§€ë‚˜ë©´ ë§Œë£Œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
        ì¼ì • ì‹œê°„(30ë¶„)ë§ˆë‹¤ ë©”ì¸ í˜ì´ì§€ë¥¼ ë‹¤ì‹œ ë°©ë¬¸í•˜ì—¬ ì¿ í‚¤ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.
        """
        # 30ë¶„(1800ì´ˆ)ë§ˆë‹¤ ì¿ í‚¤ ê°±ì‹ 
        cookie_lifetime = 1800  # 30ë¶„
        current_time = time.time()
        
        if not self.cookies_received or (current_time - self.last_cookie_refresh) > cookie_lifetime:
            logger.info("ğŸ”„ ì¿ í‚¤ ë§Œë£Œ ë˜ëŠ” ë¯¸ìˆ˜ì‹  â†’ ë©”ì¸ í˜ì´ì§€ ì¬ë°©ë¬¸...")
            self._visit_homepage()
    
    def _visit_landing_page(self, page_type: str):
        """
        âœ… API í˜¸ì¶œ ì „ì— í•´ë‹¹ í˜ì´ì§€ë¥¼ ë¨¼ì € ë°©ë¬¸ (ëœë”© í˜ì´ì§€ ì „ëµ)
        
        Selenium ì‚¬ìš© ì‹œ: ì‹¤ì œ ë¸Œë¼ìš°ì €ë¡œ ë°©ë¬¸í•˜ì—¬ ì¿ í‚¤ ê°±ì‹ 
        requests ì‚¬ìš© ì‹œ: ê¸°ì¡´ ë°©ì‹
        
        Args:
            page_type: 'complexes' (ë‹¨ì§€ ëª©ë¡), 'complex' (ë‹¨ì§€ ìƒì„¸), 'articles' (ë§¤ë¬¼ ëª©ë¡)
        """
        landing_urls = {
            'complexes': 'https://new.land.naver.com/complexes',
            'complex': 'https://new.land.naver.com/complexes',
            'articles': 'https://new.land.naver.com/articles',
        }
        
        landing_url = landing_urls.get(page_type, self.BASE_URL)
        
        if self.use_selenium and self.driver:
            # âœ… Seleniumìœ¼ë¡œ í˜ì´ì§€ ë°©ë¬¸ (ì¿ í‚¤ ê°±ì‹ )
            try:
                logger.info(f"ğŸšª Seleniumìœ¼ë¡œ ëœë”© í˜ì´ì§€ ë°©ë¬¸: {landing_url}")
                
                self.driver.get(landing_url)
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                WebDriverWait(self.driver, 5).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                # ì¿ í‚¤ ê°±ì‹ 
                selenium_cookies = self.driver.get_cookies()
                for cookie in selenium_cookies:
                    self.session.cookies.set(cookie['name'], cookie['value'])
                
                time.sleep(random.uniform(0.5, 1.5))
                
            except Exception as e:
                logger.warning(f"âš ï¸  Selenium ëœë”© í˜ì´ì§€ ë°©ë¬¸ ì‹¤íŒ¨: {e}")
        
        else:
            # âŒ requestsë¡œ í˜ì´ì§€ ë°©ë¬¸
            try:
                logger.info(f"ğŸšª ëœë”© í˜ì´ì§€ ë°©ë¬¸: {landing_url}")
                
                # Accept í—¤ë”ë¥¼ HTML í˜ì´ì§€ìš©ìœ¼ë¡œ ë³€ê²½
                original_accept = self.session.headers.get('Accept', '')
                self.session.headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8'
                
                # í˜ì´ì§€ ë°©ë¬¸
                self.session.get(landing_url, timeout=10)
                
                # Accept í—¤ë” ë³µì› (API ìš”ì²­ìš©)
                if original_accept:
                    self.session.headers['Accept'] = original_accept
                
                # ì§§ì€ ëŒ€ê¸° (0.5-1.5ì´ˆ)
                time.sleep(random.uniform(0.5, 1.5))
                
            except Exception as e:
                logger.warning(f"âš ï¸  ëœë”© í˜ì´ì§€ ë°©ë¬¸ ì‹¤íŒ¨: {e}")
    
    def _get_referer_for_url(self, url: str) -> str:
        """
        URLì— ë”°ë¼ ì ì ˆí•œ Referer ë°˜í™˜ (Referer ì²´ì¸)
        
        Args:
            url: ìš”ì²­ URL
            
        Returns:
            ì ì ˆí•œ Referer URL
        """
        if '/api/complexes' in url:
            # ë‹¨ì§€ ê²€ìƒ‰ API â†’ ë©”ì¸ í˜ì´ì§€ì—ì„œ ì˜¨ ê²ƒì²˜ëŸ¼
            return 'https://new.land.naver.com/'
        elif '/api/articles/complex/' in url:
            # ë§¤ë¬¼ ëª©ë¡ API â†’ ë‹¨ì§€ í˜ì´ì§€ì—ì„œ ì˜¨ ê²ƒì²˜ëŸ¼
            return 'https://new.land.naver.com/complexes'
        elif '/api/articles/' in url:
            # ë§¤ë¬¼ ìƒì„¸ API â†’ ë§¤ë¬¼ ëª©ë¡ì—ì„œ ì˜¨ ê²ƒì²˜ëŸ¼
            return 'https://new.land.naver.com/articles'
        else:
            # ê¸°ë³¸ê°’
            return 'https://new.land.naver.com/'
    
    def _safe_request(self, url: str, params: Dict = None, retry: int = 3) -> Optional[Dict]:
        """
        ì•ˆì „í•œ HTTP ìš”ì²­ (ì¬ì‹œë„ í¬í•¨, 429 ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬, ì‚¬ëŒì²˜ëŸ¼ í–‰ë™)
        
        Args:
            url: ìš”ì²­ URL
            params: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
            retry: ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            JSON ì‘ë‹µ ë˜ëŠ” None
        """
        # ì¿ í‚¤ ìœ íš¨ì„± ê²€ì‚¬ ë° ê°±ì‹ 
        self._check_and_refresh_cookies()
        
        # ìš”ì²­ ì „ íœ´ì‹ í•„ìš” ì—¬ë¶€ í™•ì¸
        if self._should_take_break():
            self._take_break()
        
        for attempt in range(retry):
            try:
                # âœ… ê°œì„ : User-AgentëŠ” ì„¸ì…˜ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì„¤ì •í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ë³€ê²½í•˜ì§€ ì•ŠìŒ!
                # Refererë§Œ URLì— ë§ê²Œ ë™ì ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
                referer = self._get_referer_for_url(url)
                self.session.headers['Referer'] = referer
                
                # ì¿ í‚¤ ìƒíƒœ ë¡œê¹… (ë””ë²„ê¹…ìš©)
                if self.request_count % 10 == 0:  # 10ë²ˆë§ˆë‹¤
                    cookies_count = len(self.session.cookies.get_dict())
                    logger.info(f"ğŸª í˜„ì¬ ì¿ í‚¤ ìˆ˜: {cookies_count}ê°œ")
                
                # âœ… ê°œì„ : 2ë°° ë¹ ë¥¸ ëŒ€ê¸° ì‹œê°„ (1-3ë¶„ â†’ 0.5-1.5ë¶„)
                if attempt == 0:
                    delay = self._human_like_delay(0.5, 1.5)  # 0.5-1.5ë¶„ (2ë°° ë¹ ë¦„!)
                    delay_minutes = delay / 60
                    logger.info(f"ğŸ¤” ìƒê°í•˜ëŠ” ì¤‘... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
                    time.sleep(delay)
                
                # ìš”ì²­ ì¹´ìš´íŠ¸ ì¦ê°€ ë° í”¼ë¡œë„ ì—…ë°ì´íŠ¸
                self.request_count += 1
                self._update_fatigue()
                
                response = self.session.get(url, params=params, timeout=30)
                
                if response.status_code == 200:
                    # ì„±ê³µ ì‹œ í˜ì´ì§€ ì½ê¸° ì‹œë®¬ë ˆì´ì…˜ (30% í™•ë¥ )
                    if random.random() < 0.3:
                        self._simulate_reading()
                    return response.json()
                
                elif response.status_code == 429:
                    # 429 Too Many Requests - 30ë¶„ ëŒ€ê¸°!
                    wait_minutes = 30
                    wait_seconds = wait_minutes * 60
                    
                    logger.warning(f"âš ï¸  429 ì—ëŸ¬ (Too Many Requests) ë°œìƒ!")
                    logger.info(f"ğŸš¨ í¬ë¡¤ë§ ì°¨ë‹¨ ê°ì§€ - {wait_minutes}ë¶„ ({wait_seconds}ì´ˆ) ëŒ€ê¸°...")
                    logger.info(f"   í˜„ì¬ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    logger.info(f"   ì¬ê°œ ì˜ˆì •: {(datetime.now().timestamp() + wait_seconds)}")
                    
                    if attempt < retry - 1:
                        logger.info(f"ğŸ• {wait_minutes}ë¶„ íœ´ì‹ í›„ ì¬ì‹œë„ ì˜ˆì • ({attempt + 2}/{retry})")
                        time.sleep(wait_seconds)
                        self._update_headers()  # User-Agent ë³€ê²½
                        logger.info("âœ… íœ´ì‹ ì™„ë£Œ. í¬ë¡¤ë§ ì¬ê°œ...")
                    else:
                        logger.error("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. í”„ë¡œê·¸ë¨ì„ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
                        return None
                
                elif response.status_code == 403:
                    # 403 Forbidden
                    logger.warning(f"ì ‘ê·¼ ê±°ë¶€ (403). í—¤ë” ë³€ê²½ í›„ ì¬ì‹œë„... ({attempt + 1}/{retry})")
                    self._update_headers()
                    delay = random.uniform(5, 10)
                    time.sleep(delay)
                
                else:
                    logger.warning(f"ì‘ë‹µ ì½”ë“œ {response.status_code}")
                    if attempt < retry - 1:
                        delay = random.uniform(3, 7)
                        time.sleep(delay)
                
            except requests.exceptions.RequestException as e:
                logger.error(f"ìš”ì²­ ì˜¤ë¥˜: {e}")
                if attempt < retry - 1:
                    delay = random.uniform(5, 10)
                    logger.info(f"ì˜¤ë¥˜ í›„ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(delay)
        
        return None
    
    def search_complexes(self, cortarNo: str, trade_type: str = "A1") -> List[Dict]:
        """
        ì§€ì—­ë³„ ë‹¨ì§€ ê²€ìƒ‰
        
        Args:
            cortarNo: ì§€ì—­ ì½”ë“œ (ì˜ˆ: 1168010600 - ê°•ë‚¨êµ¬ ëŒ€ì¹˜ë™)
            trade_type: ê±°ë˜ ìœ í˜• (A1: ë§¤ë§¤, B1: ì „ì„¸, B2: ì›”ì„¸, B3: ë‹¨ê¸°ì„ëŒ€)
            
        Returns:
            ë‹¨ì§€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        # API í˜¸ì¶œ ì „ ëœë”© í˜ì´ì§€ ë¨¼ì € ë°©ë¬¸ (ì¤‘ìš”!)
        self._visit_landing_page('complexes')
        
        url = f"{self.BASE_URL}/api/complexes"
        
        params = {
            'cortarNo': cortarNo,
            'realEstateType': 'APT:OPST',  # ì•„íŒŒíŠ¸, ì˜¤í”¼ìŠ¤í…”
            'tradeType': trade_type,
            'tag': '::::::::',
            'rentPriceMin': 0,
            'rentPriceMax': 999999,
            'priceMin': 0,
            'priceMax': 999999,
            'areaMin': 0,
            'areaMax': 999999,
            'oldBuildYears': '',
            'recentlyBuildYears': '',
            'minHouseHoldCount': '',
            'maxHouseHoldCount': '',
            'showArticle': 'false',
            'sameAddressGroup': 'true',
            'page': 1,
            'complexNo': '',
            'buildingNo': ''
        }
        
        logger.info(f"ë‹¨ì§€ ê²€ìƒ‰: cortarNo={cortarNo}, tradeType={trade_type}")
        data = self._safe_request(url, params)
        
        if data and 'complexList' in data:
            complexes = data['complexList']
            logger.info(f"ê²€ìƒ‰ëœ ë‹¨ì§€ ìˆ˜: {len(complexes)}")
            return complexes
        
        logger.warning("ë‹¨ì§€ ê²€ìƒ‰ ì‹¤íŒ¨")
        return []
    
    def get_complex_articles(self, complex_no: str, trade_type: str = "A1") -> List[Dict]:
        """
        íŠ¹ì • ë‹¨ì§€ì˜ ë§¤ë¬¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            complex_no: ë‹¨ì§€ ë²ˆí˜¸
            trade_type: ê±°ë˜ ìœ í˜•
            
        Returns:
            ë§¤ë¬¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        # API í˜¸ì¶œ ì „ ëœë”© í˜ì´ì§€ ë¨¼ì € ë°©ë¬¸ (ì¤‘ìš”!)
        self._visit_landing_page('complex')
        
        url = f"{self.BASE_URL}/api/articles/complex/{complex_no}"
        
        params = {
            'realEstateType': 'APT:OPST',
            'tradeType': trade_type,
            'tag': '::::::::',
            'rentPriceMin': 0,
            'rentPriceMax': 999999,
            'priceMin': 0,
            'priceMax': 999999,
            'areaMin': 0,
            'areaMax': 999999,
            'oldBuildYears': '',
            'recentlyBuildYears': '',
            'minHouseHoldCount': '',
            'maxHouseHoldCount': '',
            'showArticle': 'true',
            'sameAddressGroup': 'false',
            'minMoveInMonth': '',
            'maxMoveInMonth': '',
            'page': 1
        }
        
        logger.info(f"ë§¤ë¬¼ ê²€ìƒ‰: complexNo={complex_no}")
        data = self._safe_request(url, params)
        
        if data and 'articleList' in data:
            articles = data['articleList']
            logger.info(f"ê²€ìƒ‰ëœ ë§¤ë¬¼ ìˆ˜: {len(articles)}")
            
            # âœ… ê°œì„ : 2ë°° ë¹ ë¥¸ ëŒ€ê¸° ì‹œê°„ (2-5ë¶„ â†’ 1-2.5ë¶„)
            delay = self._human_like_delay(1.0, 2.5)  # 1-2.5ë¶„ (2ë°° ë¹ ë¦„!)
            delay_minutes = delay / 60
            logger.info(f"ğŸ•’ ë§¤ë¬¼ ëª©ë¡ í™•ì¸ ì¤‘... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
            time.sleep(delay)
            
            return articles
        
        logger.warning(f"ë§¤ë¬¼ ê²€ìƒ‰ ì‹¤íŒ¨: complexNo={complex_no}")
        return []
    
    def get_article_detail(self, article_no: str) -> Optional[Dict]:
        """
        ë§¤ë¬¼ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            article_no: ë§¤ë¬¼ ë²ˆí˜¸
            
        Returns:
            ë§¤ë¬¼ ìƒì„¸ ì •ë³´
        """
        url = f"{self.BASE_URL}/api/articles/{article_no}"
        
        logger.info(f"ë§¤ë¬¼ ìƒì„¸ ì •ë³´: articleNo={article_no}")
        data = self._safe_request(url)
        
        # âœ… ê°œì„ : 2ë°° ë¹ ë¥¸ ëŒ€ê¸° ì‹œê°„ (1-3ë¶„ â†’ 0.5-1.5ë¶„)
        delay = self._human_like_delay(0.5, 1.5)  # 0.5-1.5ë¶„ (2ë°° ë¹ ë¦„!)
        delay_minutes = delay / 60
        logger.info(f"ğŸ“„ ìƒì„¸ ì •ë³´ ì½ëŠ” ì¤‘... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
        time.sleep(delay)
        
        return data
    
    def scrape_region(self, cortarNo: str, trade_types: List[str] = ["A1"]) -> List[Dict]:
        """
        íŠ¹ì • ì§€ì—­ì˜ ëª¨ë“  ë§¤ë¬¼ í¬ë¡¤ë§
        
        Args:
            cortarNo: ì§€ì—­ ì½”ë“œ
            trade_types: ê±°ë˜ ìœ í˜• ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ëª¨ë“  ë§¤ë¬¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        all_properties = []
        
        for idx, trade_type in enumerate(trade_types):
            logger.info(f"=== ê±°ë˜ ìœ í˜• {trade_type} í¬ë¡¤ë§ ì‹œì‘ ===")
            logger.info(f"ğŸ“Š ì§„í–‰ ìƒí™©: {idx + 1}/{len(trade_types)}, ì´ ìš”ì²­: {self.request_count}íšŒ, í”¼ë¡œë„: {self.fatigue_level:.2f}")
            
            # âœ… ê°œì„ : 2ë°° ë¹ ë¥¸ ê±°ë˜ ì „í™˜ íœ´ì‹ (30-60ë¶„ â†’ 15-30ë¶„)
            if idx > 0:
                long_delay = self._human_like_delay(15.0, 30.0)  # 15-30ë¶„ (2ë°° ë¹ ë¦„!)
                long_delay_minutes = long_delay / 60
                logger.info(f"ğŸ”„ ê±°ë˜ ìœ í˜• ì „í™˜ íœ´ì‹: {long_delay_minutes:.1f}ë¶„ ({long_delay:.0f}ì´ˆ)")
                time.sleep(long_delay)
            
            # 1. ë‹¨ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            complexes = self.search_complexes(cortarNo, trade_type)
            
            # ìˆœì„œ ë¬´ì‘ìœ„í™” (Shuffle) - ì‚¬ëŒì²˜ëŸ¼ ë¶ˆê·œì¹™í•˜ê²Œ!
            if complexes:
                random.shuffle(complexes)
                logger.info(f"ğŸ”€ ë‹¨ì§€ ìˆœì„œ ë¬´ì‘ìœ„í™” ì™„ë£Œ (ì´ {len(complexes)}ê°œ)")
            
            # 2. ê° ë‹¨ì§€ì˜ ë§¤ë¬¼ ê°€ì ¸ì˜¤ê¸°
            for i, complex_info in enumerate(complexes[:10], 1):  # í…ŒìŠ¤íŠ¸: ìƒìœ„ 10ê°œë§Œ
                complex_no = complex_info.get('complexNo')
                complex_name = complex_info.get('complexName', 'ì•Œ ìˆ˜ ì—†ìŒ')
                
                logger.info(f"[{i}/{len(complexes[:10])}] {complex_name} (complexNo: {complex_no})")
                
                # âœ… ê°œì„ : requestsì—ì„œëŠ” ë§ˆìš°ìŠ¤ ì‹œë®¬ë ˆì´ì…˜ì´ ì˜ë¯¸ ì—†ìœ¼ë¯€ë¡œ ì œê±°
                
                articles = self.get_complex_articles(complex_no, trade_type)
                
                # ë§¤ë¬¼ ìˆœì„œë„ ë¬´ì‘ìœ„í™” (Shuffle)
                if articles:
                    random.shuffle(articles)
                    logger.info(f"ğŸ”€ ë§¤ë¬¼ ìˆœì„œ ë¬´ì‘ìœ„í™” ì™„ë£Œ (ì´ {len(articles)}ê°œ)")
                
                for article in articles:
                    # ë§¤ë¬¼ ë°ì´í„° ê°€ê³µ
                    property_data = self._parse_article(article, complex_info, trade_type)
                    all_properties.append(property_data)
                
                # âœ… ê°œì„ : 2ë°° ë¹ ë¥¸ ë‹¨ì§€ ì´ë™ (5-10ë¶„ â†’ 2.5-5ë¶„)
                delay = self._human_like_delay(2.5, 5.0)  # 2.5-5ë¶„ (2ë°° ë¹ ë¦„!)
                delay_minutes = delay / 60
                logger.info(f"ğŸ¢ ë‹¤ìŒ ë‹¨ì§€ë¡œ ì´ë™... {delay_minutes:.1f}ë¶„ ({delay:.0f}ì´ˆ)")
                time.sleep(delay)
                
                # âœ… ê°œì„ : 2ë°° ë¹ ë¥¸ ì¶”ê°€ íœ´ì‹ (15-30ë¶„ â†’ 7.5-15ë¶„)
                if random.random() < 0.2:
                    long_break_minutes = random.uniform(7.5, 15)
                    long_break_seconds = long_break_minutes * 60
                    logger.info(f"ğŸ’¤ ì¶”ê°€ ì¥ì‹œê°„ íœ´ì‹: {long_break_minutes:.1f}ë¶„ ({long_break_seconds:.0f}ì´ˆ)")
                    time.sleep(long_break_seconds)
        
        logger.info(f"ì´ {len(all_properties)}ê°œ ë§¤ë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")
        return all_properties
    
    def _parse_article(self, article: Dict, complex_info: Dict, trade_type: str) -> Dict:
        """
        ë§¤ë¬¼ ë°ì´í„° íŒŒì‹±
        
        Args:
            article: ë§¤ë¬¼ ì›ë³¸ ë°ì´í„°
            complex_info: ë‹¨ì§€ ì •ë³´
            trade_type: ê±°ë˜ ìœ í˜•
            
        Returns:
            íŒŒì‹±ëœ ë§¤ë¬¼ ì •ë³´
        """
        article_no = article.get('articleNo', '')
        complex_no = complex_info.get('complexNo', '')
        
        return {
            'id': f"{complex_no}_{article_no}",
            'complex_no': complex_no,
            'complex_name': complex_info.get('complexName', ''),
            'article_no': article_no,
            'price': article.get('dealOrWarrantPrc', 0),  # ë§¤ë§¤ê°€ ë˜ëŠ” ì „ì„¸ê°€
            'area_real': article.get('area1', 0),  # ê³µê¸‰ë©´ì 
            'area_exclusive': article.get('area2', 0),  # ì „ìš©ë©´ì 
            'floor': article.get('floorInfo', ''),
            'total_floors': complex_info.get('maxFloor', 0),
            'direction': article.get('direction', ''),
            'trade_type': trade_type,
            'approval_year': complex_info.get('useApproveYmd', '')[:4] if complex_info.get('useApproveYmd') else 0,
            'household_count': complex_info.get('totalHouseholdCount', 0),
            'room_count': article.get('roomCnt', 0),
            'bathroom_count': article.get('bathroomCnt', 0),
            'loan_amount': article.get('loanAmount', 0),
            'description': article.get('tagList', []),
            'url': f"https://new.land.naver.com/complexes/{complex_no}?articleNo={article_no}"
        }


    def __del__(self):
        """ì†Œë©¸ì: Selenium WebDriver ì¢…ë£Œ"""
        if self.driver:
            try:
                self.driver.quit()
                logger.info("âœ… Selenium WebDriver ì¢…ë£Œë¨")
            except:
                pass


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    scraper = NaverRealEstateScraper(use_selenium=True)  # âœ… Selenium ì‚¬ìš©!
    
    # ê°•ë‚¨êµ¬ ëŒ€ì¹˜ë™ ì§€ì—­ ì½”ë“œ
    cortarNo = "1168010600"
    
    try:
        properties = scraper.scrape_region(cortarNo, trade_types=["B1"])
        
        print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {len(properties)}ê°œ ë§¤ë¬¼")
        if properties:
            print("\nì²« ë²ˆì§¸ ë§¤ë¬¼ ì˜ˆì‹œ:")
            print(properties[0])
    
    finally:
        # Selenium ì¢…ë£Œ
        if scraper.driver:
            scraper.driver.quit()
