"""
ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ë§ ëª¨ë“ˆ
APIë¥¼ í†µí•´ ë§¤ë¬¼ ì •ë³´ ìˆ˜ì§‘
"""

import requests
import random
import time
from typing import List, Dict, Optional
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NaverRealEstateScraper:
    """ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    # ë‹¤ì–‘í•œ User-Agent ë¦¬ìŠ¤íŠ¸ (ì°¨ë‹¨ íšŒí”¼) - ë” ë‹¤ì–‘í•˜ê²Œ!
    USER_AGENTS = [
        # Chrome (Windows)
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        # Chrome (Mac)
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        # Firefox (Windows)
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        # Firefox (Mac)
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0',
        # Edge
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',
        # Safari (Mac)
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
    ]
    
    BASE_URL = "https://new.land.naver.com"
    
    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self._update_headers()
        self._visit_homepage()  # ì´ˆê¸° ë°©ë¬¸ìœ¼ë¡œ ì¿ í‚¤ ë°›ê¸°
    
    def _visit_homepage(self):
        """
        ë„¤ì´ë²„ ë¶€ë™ì‚° í™ˆí˜ì´ì§€ ë°©ë¬¸ (ì¿ í‚¤ ë°›ê¸°)
        ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë™ì‘í•˜ê¸° ìœ„í•´
        """
        try:
            logger.info("ë„¤ì´ë²„ ë¶€ë™ì‚° í™ˆí˜ì´ì§€ ë°©ë¬¸ ì¤‘...")
            self.session.get(self.BASE_URL, timeout=10)
            time.sleep(random.uniform(2, 4))
            logger.info("ì´ˆê¸° ë°©ë¬¸ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ì´ˆê¸° ë°©ë¬¸ ì‹¤íŒ¨: {e}")
    
    def _update_headers(self):
        """ìš”ì²­ í—¤ë” ì—…ë°ì´íŠ¸ (ì°¨ë‹¨ íšŒí”¼)"""
        self.session.headers.update({
            'User-Agent': random.choice(self.USER_AGENTS),
            'Referer': 'https://new.land.naver.com/',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
    
    def _safe_request(self, url: str, params: Dict = None, retry: int = 3) -> Optional[Dict]:
        """
        ì•ˆì „í•œ HTTP ìš”ì²­ (ì¬ì‹œë„ í¬í•¨, 429 ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬)
        
        Args:
            url: ìš”ì²­ URL
            params: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
            retry: ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            JSON ì‘ë‹µ ë˜ëŠ” None
        """
        for attempt in range(retry):
            try:
                # ê¸°ë³¸ ìš”ì²­ ì „ ë”œë ˆì´ (ì†ë„ ì œí•œ)
                if attempt == 0:
                    time.sleep(random.uniform(1, 2))
                
                response = self.session.get(url, params=params, timeout=30)
                
                if response.status_code == 200:
                    return response.json()
                
                elif response.status_code == 429:
                    # 429 Too Many Requests - íŠ¹ë³„ ì²˜ë¦¬!
                    # ì§€ìˆ˜ ë°±ì˜¤í”„: ì‹œë„ë§ˆë‹¤ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                    wait_time = random.uniform(30, 60) * (2 ** attempt)  # 30ì´ˆ â†’ 60ì´ˆ â†’ 120ì´ˆ
                    logger.warning(f"âš ï¸  429 ì—ëŸ¬ (Too Many Requests)")
                    logger.info(f"ğŸ“¢ ê¶Œì¥ ëŒ€ê¸° ì‹œê°„: {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                    
                    if attempt < retry - 1:
                        logger.info(f"ì¬ì‹œë„ ì˜ˆì • ({attempt + 2}/{retry})")
                        time.sleep(wait_time)
                        self._update_headers()  # User-Agent ë³€ê²½
                    else:
                        logger.error("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                        return None
                
                elif response.status_code == 403:
                    # 403 Forbidden
                    logger.warning(f"ì ‘ê·¼ ê±°ë¶€ (403). í—¤ë” ë³€ê²½ í›„ ì¬ì‹œë„... ({attempt + 1}/{retry})")
                    self._update_headers()
                    delay = random.uniform(5, 10)
                    time.sleep(delay)
                
                else:
                    logger.warning(f"ì‘ë‹µ ì½”ë“œ {response.status_code}")
                    if attempt < retry - 1:
                        delay = random.uniform(3, 7)
                        time.sleep(delay)
                
            except requests.exceptions.RequestException as e:
                logger.error(f"ìš”ì²­ ì˜¤ë¥˜: {e}")
                if attempt < retry - 1:
                    delay = random.uniform(5, 10)
                    logger.info(f"ì˜¤ë¥˜ í›„ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(delay)
        
        return None
    
    def search_complexes(self, cortarNo: str, trade_type: str = "A1") -> List[Dict]:
        """
        ì§€ì—­ë³„ ë‹¨ì§€ ê²€ìƒ‰
        
        Args:
            cortarNo: ì§€ì—­ ì½”ë“œ (ì˜ˆ: 1168010600 - ê°•ë‚¨êµ¬ ëŒ€ì¹˜ë™)
            trade_type: ê±°ë˜ ìœ í˜• (A1: ë§¤ë§¤, B1: ì „ì„¸, B2: ì›”ì„¸, B3: ë‹¨ê¸°ì„ëŒ€)
            
        Returns:
            ë‹¨ì§€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        url = f"{self.BASE_URL}/api/complexes"
        
        params = {
            'cortarNo': cortarNo,
            'realEstateType': 'APT:OPST',  # ì•„íŒŒíŠ¸, ì˜¤í”¼ìŠ¤í…”
            'tradeType': trade_type,
            'tag': '::::::::',
            'rentPriceMin': 0,
            'rentPriceMax': 999999,
            'priceMin': 0,
            'priceMax': 999999,
            'areaMin': 0,
            'areaMax': 999999,
            'oldBuildYears': '',
            'recentlyBuildYears': '',
            'minHouseHoldCount': '',
            'maxHouseHoldCount': '',
            'showArticle': 'false',
            'sameAddressGroup': 'true',
            'page': 1,
            'complexNo': '',
            'buildingNo': ''
        }
        
        logger.info(f"ë‹¨ì§€ ê²€ìƒ‰: cortarNo={cortarNo}, tradeType={trade_type}")
        data = self._safe_request(url, params)
        
        if data and 'complexList' in data:
            complexes = data['complexList']
            logger.info(f"ê²€ìƒ‰ëœ ë‹¨ì§€ ìˆ˜: {len(complexes)}")
            return complexes
        
        logger.warning("ë‹¨ì§€ ê²€ìƒ‰ ì‹¤íŒ¨")
        return []
    
    def get_complex_articles(self, complex_no: str, trade_type: str = "A1") -> List[Dict]:
        """
        íŠ¹ì • ë‹¨ì§€ì˜ ë§¤ë¬¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            complex_no: ë‹¨ì§€ ë²ˆí˜¸
            trade_type: ê±°ë˜ ìœ í˜•
            
        Returns:
            ë§¤ë¬¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        url = f"{self.BASE_URL}/api/articles/complex/{complex_no}"
        
        params = {
            'realEstateType': 'APT:OPST',
            'tradeType': trade_type,
            'tag': '::::::::',
            'rentPriceMin': 0,
            'rentPriceMax': 999999,
            'priceMin': 0,
            'priceMax': 999999,
            'areaMin': 0,
            'areaMax': 999999,
            'oldBuildYears': '',
            'recentlyBuildYears': '',
            'minHouseHoldCount': '',
            'maxHouseHoldCount': '',
            'showArticle': 'true',
            'sameAddressGroup': 'false',
            'minMoveInMonth': '',
            'maxMoveInMonth': '',
            'page': 1
        }
        
        logger.info(f"ë§¤ë¬¼ ê²€ìƒ‰: complexNo={complex_no}")
        data = self._safe_request(url, params)
        
        if data and 'articleList' in data:
            articles = data['articleList']
            logger.info(f"ê²€ìƒ‰ëœ ë§¤ë¬¼ ìˆ˜: {len(articles)}")
            
            # ëœë¤ ë”œë ˆì´ (ì°¨ë‹¨ íšŒí”¼) - ë” ê¸¸ê²Œ!
            time.sleep(random.uniform(3, 6))
            
            return articles
        
        logger.warning(f"ë§¤ë¬¼ ê²€ìƒ‰ ì‹¤íŒ¨: complexNo={complex_no}")
        return []
    
    def get_article_detail(self, article_no: str) -> Optional[Dict]:
        """
        ë§¤ë¬¼ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            article_no: ë§¤ë¬¼ ë²ˆí˜¸
            
        Returns:
            ë§¤ë¬¼ ìƒì„¸ ì •ë³´
        """
        url = f"{self.BASE_URL}/api/articles/{article_no}"
        
        logger.info(f"ë§¤ë¬¼ ìƒì„¸ ì •ë³´: articleNo={article_no}")
        data = self._safe_request(url)
        
        # ëœë¤ ë”œë ˆì´ - ë” ê¸¸ê²Œ!
        time.sleep(random.uniform(2, 4))
        
        return data
    
    def scrape_region(self, cortarNo: str, trade_types: List[str] = ["A1"]) -> List[Dict]:
        """
        íŠ¹ì • ì§€ì—­ì˜ ëª¨ë“  ë§¤ë¬¼ í¬ë¡¤ë§
        
        Args:
            cortarNo: ì§€ì—­ ì½”ë“œ
            trade_types: ê±°ë˜ ìœ í˜• ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ëª¨ë“  ë§¤ë¬¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        all_properties = []
        
        for trade_type in trade_types:
            logger.info(f"=== ê±°ë˜ ìœ í˜• {trade_type} í¬ë¡¤ë§ ì‹œì‘ ===")
            
            # 1. ë‹¨ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            complexes = self.search_complexes(cortarNo, trade_type)
            
            # 2. ê° ë‹¨ì§€ì˜ ë§¤ë¬¼ ê°€ì ¸ì˜¤ê¸°
            for i, complex_info in enumerate(complexes[:10], 1):  # í…ŒìŠ¤íŠ¸: ìƒìœ„ 10ê°œë§Œ
                complex_no = complex_info.get('complexNo')
                complex_name = complex_info.get('complexName', 'ì•Œ ìˆ˜ ì—†ìŒ')
                
                logger.info(f"[{i}/{len(complexes[:10])}] {complex_name} (complexNo: {complex_no})")
                
                articles = self.get_complex_articles(complex_no, trade_type)
                
                for article in articles:
                    # ë§¤ë¬¼ ë°ì´í„° ê°€ê³µ
                    property_data = self._parse_article(article, complex_info, trade_type)
                    all_properties.append(property_data)
                
                # ë‹¨ì§€ ê°„ ë”œë ˆì´ - ë” ê¸¸ê²Œ! (ì°¨ë‹¨ ë°©ì§€)
                delay = random.uniform(5, 10)
                logger.info(f"ë‹¤ìŒ ë‹¨ì§€ í¬ë¡¤ë§ ì „ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(delay)
        
        logger.info(f"ì´ {len(all_properties)}ê°œ ë§¤ë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")
        return all_properties
    
    def _parse_article(self, article: Dict, complex_info: Dict, trade_type: str) -> Dict:
        """
        ë§¤ë¬¼ ë°ì´í„° íŒŒì‹±
        
        Args:
            article: ë§¤ë¬¼ ì›ë³¸ ë°ì´í„°
            complex_info: ë‹¨ì§€ ì •ë³´
            trade_type: ê±°ë˜ ìœ í˜•
            
        Returns:
            íŒŒì‹±ëœ ë§¤ë¬¼ ì •ë³´
        """
        article_no = article.get('articleNo', '')
        complex_no = complex_info.get('complexNo', '')
        
        return {
            'id': f"{complex_no}_{article_no}",
            'complex_no': complex_no,
            'complex_name': complex_info.get('complexName', ''),
            'article_no': article_no,
            'price': article.get('dealOrWarrantPrc', 0),  # ë§¤ë§¤ê°€ ë˜ëŠ” ì „ì„¸ê°€
            'area_real': article.get('area1', 0),  # ê³µê¸‰ë©´ì 
            'area_exclusive': article.get('area2', 0),  # ì „ìš©ë©´ì 
            'floor': article.get('floorInfo', ''),
            'total_floors': complex_info.get('maxFloor', 0),
            'direction': article.get('direction', ''),
            'trade_type': trade_type,
            'approval_year': complex_info.get('useApproveYmd', '')[:4] if complex_info.get('useApproveYmd') else 0,
            'household_count': complex_info.get('totalHouseholdCount', 0),
            'room_count': article.get('roomCnt', 0),
            'bathroom_count': article.get('bathroomCnt', 0),
            'loan_amount': article.get('loanAmount', 0),
            'description': article.get('tagList', []),
            'url': f"https://new.land.naver.com/complexes/{complex_no}?articleNo={article_no}"
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    scraper = NaverRealEstateScraper()
    
    # ê°•ë‚¨êµ¬ ëŒ€ì¹˜ë™ ì§€ì—­ ì½”ë“œ
    cortarNo = "1168010600"
    
    properties = scraper.scrape_region(cortarNo, trade_types=["A1"])
    
    print(f"\ní¬ë¡¤ë§ ì™„ë£Œ: {len(properties)}ê°œ ë§¤ë¬¼")
    if properties:
        print("\nì²« ë²ˆì§¸ ë§¤ë¬¼ ì˜ˆì‹œ:")
        print(properties[0])
